{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "from collections import defaultdict\n",
    "\n",
    "def iterate_children(child, parent_name='model', depth=1):\n",
    "    if depth > 1:\n",
    "        children_list = []\n",
    "        for name, grandchild in child.named_children():\n",
    "            children_list += iterate_children(grandchild, parent_name+'.'+name, depth-1)\n",
    "        return children_list\n",
    "    else:\n",
    "        return {(parent_name+'.'+name, module) for name, module in child.named_children()}\n",
    "\n",
    "def store_activations(activations_dict, layer_name, module, input, output):\n",
    "    activations_dict[layer_name] = output\n",
    "\n",
    "def hook_model(model, depth):\n",
    "    model.activations = defaultdict(list)\n",
    "    for layer_name, child in iterate_children(model, depth=depth):\n",
    "        child.register_forward_hook(partial(store_activations, model.activations, layer_name))\n",
    "    return model\n",
    "\n",
    "def choose_downsampling(activations, max_fs):\n",
    "    if activations.ndim == 5:\n",
    "        activations = activations[0:1]\n",
    "        test_range = activations.shape[-1]\n",
    "        numels = np.zeros((test_range+1, test_range))\n",
    "        pbar = tqdm(range(sum(range(test_range+1))))\n",
    "        for k in range(1,test_range+1):\n",
    "            for s in range(1,k+1):\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix_str(f\"testing size {k}, stride {s}\")\n",
    "                n = (activations.shape[-1] - k) / s\n",
    "                if n != int(n):\n",
    "                    continue\n",
    "                else:\n",
    "                    pooled = torch.nn.functional.max_pool3d(activations, kernel_size=(2,k,k), stride=s)\n",
    "                    if pooled.shape[-1] > 1 and pooled.numel() <= max_fs:\n",
    "                        numels[k,s] = pooled.numel()\n",
    "                    else:\n",
    "                        continue\n",
    "        best_k, best_s = np.unravel_index(np.argmax(numels, axis=None), numels.shape)\n",
    "        if (best_k, best_s) == (0,0):\n",
    "            return None\n",
    "        else:\n",
    "            return torch.nn.MaxPool3d(kernel_size=(2, best_k, best_k), stride=best_s)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "DEVICE = 'cuda:0'\n",
    "DTYPE = torch.float\n",
    "stimulus_shape = (3,32,112,112)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/matthew/Code/yhit\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd ~/Code/yhit/\n",
    "from modelzoo.dorsalnet import DorsalNet\n",
    "\n",
    "model = DorsalNet(False, 32).eval().to(DEVICE).to(DTYPE)\n",
    "model.load_state_dict(torch.load('/home/matthew/Data/DorsalNet_FC/base_models/DorsalNet/pretrained.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************\n",
      "model.conv1\n",
      "old_shape: torch.Size([6422528])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1596/1596 [00:01<00:00, 931.66it/s, testing size 56, stride 56] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_shape: torch.Size([1280])\n",
      "**************\n",
      "model.s1\n",
      "old_shape: torch.Size([1605632])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 406/406 [00:00<00:00, 1129.40it/s, testing size 28, stride 28]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_shape: torch.Size([1280])\n",
      "**************\n",
      "model.res0\n",
      "old_shape: torch.Size([802816])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 406/406 [00:00<00:00, 971.08it/s, testing size 28, stride 28] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_shape: torch.Size([1440])\n",
      "**************\n",
      "model.res1\n",
      "old_shape: torch.Size([802816])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 406/406 [00:00<00:00, 785.42it/s, testing size 28, stride 28]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_shape: torch.Size([1440])\n",
      "**************\n",
      "model.res2\n",
      "old_shape: torch.Size([802816])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 406/406 [00:00<00:00, 954.13it/s, testing size 28, stride 28] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_shape: torch.Size([1440])\n",
      "**************\n",
      "model.res3\n",
      "old_shape: torch.Size([802816])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 406/406 [00:00<00:00, 1004.78it/s, testing size 28, stride 28]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_shape: torch.Size([1440])\n",
      "**************\n",
      "model.concat\n",
      "old_shape: torch.Size([2408448])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 406/406 [00:00<00:00, 940.93it/s, testing size 28, stride 28]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_shape: torch.Size([1152])\n",
      "**************\n",
      "model.dropout\n",
      "old_shape: torch.Size([802816])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 406/406 [00:00<00:00, 895.12it/s, testing size 28, stride 28] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_shape: torch.Size([1440])\n"
     ]
    }
   ],
   "source": [
    "MAX_FS = 1500\n",
    "    \n",
    "model = hook_model(model, 1)\n",
    "model(torch.randn(stimulus_shape).unsqueeze(0).to(DEVICE).to(DTYPE))\n",
    "\n",
    "layer_downsampling_fns = {}\n",
    "for layer_name, layer_activations in model.activations.items():\n",
    "    print('**************')\n",
    "    print(layer_name)\n",
    "    print('old_shape:', layer_activations.flatten().shape)\n",
    "    layer_downsampling_fn = choose_downsampling(layer_activations, MAX_FS)\n",
    "    layer_downsampling_fns[layer_name] = layer_downsampling_fn\n",
    "    if layer_downsampling_fn is not None:\n",
    "        layer_activations = layer_downsampling_fns[layer_name](layer_activations)\n",
    "    print('new_shape:', layer_activations.flatten().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** vedb_ver01 ****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [08:08<00:00,  2.46it/s]\n",
      "/home/matthew/anaconda3/envs/pomlab/lib/python3.7/site-packages/ipykernel_launcher.py:52: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "100%|██████████| 90/90 [00:31<00:00,  2.89it/s]\n"
     ]
    }
   ],
   "source": [
    "interpolate_frames = torchvision.transforms.Compose([\n",
    "    torchvision.ops.Permute([1,2,3,0]),\n",
    "    torchvision.transforms.Resize([stimulus_shape[-1],stimulus_shape[1]]),\n",
    "    torchvision.ops.Permute([0,3,1,2]),\n",
    "])\n",
    "\n",
    "preprocess = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Resize(stimulus_shape[-1]),\n",
    "    torchvision.transforms.CenterCrop(stimulus_shape[-1]),\n",
    "    # torchvision.transforms.Normalize(123.0, 75.0),\n",
    "])\n",
    "\n",
    "model_name='dorsalnet'\n",
    "DTYPE=torch.float32\n",
    "iter_mode = 'children'\n",
    "iter_depth = 1\n",
    "\n",
    "save_dir = f\"/home/matthew/remote_mounts/pomcloud0/students/matthew/projects/VWAM/DNNs/{model_name}/\"\n",
    "\n",
    "batch_sizes = {\n",
    "    # 'LHimages': 1,\n",
    "    'NaturalMovies': 30,\n",
    "    'vedb_ver01': 50,\n",
    "    'BiomotionPilot06': 48,\n",
    "}\n",
    "\n",
    "for experiment in ['LHimages', 'NaturalMovies', 'vedb_ver01'][2:]:\n",
    "    print('****', experiment, '****')\n",
    "    images_dir = f'/hdd01/stimuli/{experiment}/ImageFolder'\n",
    "    for split in ['trn', 'val']:\n",
    "        dataset = datasets.ImageFolder(images_dir+'/'+split, transform=preprocess)\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_sizes[experiment], shuffle=False)\n",
    "        activations_dict = defaultdict(list)\n",
    "        for images, labels in tqdm(dataloader):\n",
    "            images = interpolate_frames(images).unsqueeze(0).to(DTYPE).to(DEVICE)\n",
    "            model(images);\n",
    "            layer_names = list(model.activations.keys())\n",
    "            for layer_name in layer_names:\n",
    "                layer_activations = model.activations[layer_name].detach().cpu()\n",
    "                del model.activations[layer_name]\n",
    "                layer_downsampling_fn = layer_downsampling_fns[layer_name]\n",
    "                if not isinstance(layer_downsampling_fn, type(None)):\n",
    "                    layer_activations = layer_downsampling_fn(layer_activations)\n",
    "                # if experiment != 'LHimages':\n",
    "                #     layer_activations = torch.mean(layer_activations, 0).unsqueeze(0)\n",
    "                activations_dict[layer_name].append(layer_activations.numpy())\n",
    "        activations_dict = {name: np.concatenate(outputs, 0) for name, outputs in activations_dict.items()}\n",
    "        if not os.path.exists(f'{save_dir}/activations/{experiment}'):\n",
    "            os.makedirs(f'{save_dir}/activations/{experiment}')\n",
    "        np.savez(f'{save_dir}/activations/{experiment}/{split}_activations.npz', **activations_dict)\n",
    "        activations_concatenated = np.nan_to_num(np.concatenate([value.reshape(len(value), -1) for value in list(activations_dict.values())], 1).astype(np.float))\n",
    "        np.save(f'{save_dir}/activations/{experiment}/{split}_activations.npy', activations_concatenated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import vm_tools as vmt\n",
    "import cortex as cx\n",
    "\n",
    "experiments = ['NaturalMovies', 'vedb_ver01']\n",
    "model_name='dorsalnet'\n",
    "\n",
    "for subject_id in [f'S0{i}' for i in range(9)]:\n",
    "    for experiment in experiments:\n",
    "        if os.path.exists(f'/home/matthew/Data/DorsalNet_FC/fMRI_data/{subject_id}/{experiment}/'):\n",
    "            fit_dir = f'/home/matthew/remote_mounts/pomcloud0/students/matthew/Projects/VWAM/regression_fits/{model_name}/{experiment}/{subject_id}'\n",
    "            # if os.path.exists(fit_dir+'/ridge.npz'):\n",
    "                # fit = np.load(fit_dir+'/ridge.npz')\n",
    "            # else:\n",
    "            trn_a = np.load(f'/home/matthew/remote_mounts/pomcloud0/students/matthew/Projects/VWAM/DNNs/{model_name}/activations/{experiment}/trn_activations.npy')\n",
    "            val_a = np.load(f'/home/matthew/remote_mounts/pomcloud0/students/matthew/Projects/VWAM/DNNs/{model_name}/activations/{experiment}/val_activations.npy')\n",
    "\n",
    "            if experiment=='NaturalMovies' and subject_id=='S01':\n",
    "                trn_a = trn_a[:2400]\n",
    "                val_a = val_a[:180]\n",
    "\n",
    "            if experiment != 'LHimages':\n",
    "                trn_a = vmt.utils.add_lags(trn_a)\n",
    "                val_a = vmt.utils.add_lags(val_a)\n",
    "            \n",
    "            if not os.path.exists(fit_dir):\n",
    "                os.makedirs(fit_dir)\n",
    "            save_path = fit_dir+f\"/ridge.npz\"\n",
    "            trn_brain = np.load(f'/home/matthew/Data/DorsalNet_FC/fMRI_data/{subject_id}/{experiment}/trn.npy')\n",
    "            val_brain = np.load(f'/home/matthew/Data/DorsalNet_FC/fMRI_data/{subject_id}/{experiment}/val_rpts.npy').mean(0)\n",
    "            fit = vmt.Regression.ridge_cv(trn_fs=trn_a, trn_data=trn_brain,\n",
    "                                                val_fs=val_a, val_data=val_brain,\n",
    "                                                alphas = list(np.logspace(0,20,20)), ## default range is much too low\n",
    "                                                select_by='individual_voxel_r2',\n",
    "                                                do_re_zscore_fs=False, do_re_zscore_data=False, is_verbose=False,\n",
    "                                                chunk_sz=100000,\n",
    "                                                )\n",
    "            if experiment != 'LHimages':\n",
    "                fit['weights_lagged'] = fit['weights'].copy()\n",
    "                fit['last_two_lags_mean'] = np.nanmean([fit['weights_lagged'][len(fit['weights_lagged'])//3:-len(fit['weights_lagged'])//3], fit['weights_lagged'][-len(fit['weights_lagged'])//3:]], axis=0)\n",
    "                fit['weights'] = vmt.utils.avg_wts(fit['weights'].T, skipfirst=False).T\n",
    "            np.savez(save_path, **fit)\n",
    "            print('mean cc:', np.nanmean(fit['cc']))\n",
    "            mask = np.load(f'/home/matthew/Data/DorsalNet_FC/fMRI_data/{subject_id}/{experiment}/mask.npy')\n",
    "            cx.webshow(cx.Volume(fit['cc'], subject=subject_id, xfmname=experiment, mask=mask, vmin=0, vmax=1, cmap='afmhot'), title=f\"{subject_id} {experiment} ccs\", with_curvature=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation maximization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Just network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skvideo\n",
    "from skvideo import io\n",
    "import torchvision\n",
    "\n",
    "model = DorsalNet(False, 32).eval().to(DEVICE).to(DTYPE)\n",
    "model.load_state_dict(torch.load('/home/matthew/Data/DorsalNet_FC/base_models/DorsalNet/pretrained.pth'))\n",
    "\n",
    "preprocess = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(112),\n",
    "    # torchvision.transforms.ToTensor(),\n",
    "    # tf.Normalize(mean=[0.43216, 0.394666, 0.37645], std=[0.22803, 0.22145, 0.216989]),\n",
    "])\n",
    "\n",
    "dl = DataLoader(ImageFolder('/home/matthew/Data/DorsalNet_FC/stimuli/NaturalMovies/images/trn', transform=preprocess), batch_size=32, shuffle=False)\n",
    "\n",
    "invariance_transforms = transforms.Compose([\n",
    "    transforms.RandomCrop((512,512), padding=5),\n",
    "    # transforms.GaussianBlur(31),\n",
    "    # transforms.RandomRotation([-5,5]),\n",
    "    # transforms.RandomResizedCrop((500,500), scale=(.95,1.05), ratio=(1,1,1)),\n",
    "    transforms.RandomCrop((512,512), padding=3),\n",
    "])\n",
    "\n",
    "lr = 1e2\n",
    "\n",
    "for dim in range(4):\n",
    "    for loc in range(0,28,8):\n",
    "        fspace = torch.randn((1,3,32,512,512), device=DEVICE, dtype=torch.complex64).requires_grad_(True)\n",
    "        optimizer = torch.optim.Adam([fspace], lr=lr)\n",
    "        iterator = tqdm(range(100))\n",
    "        for i in iterator:\n",
    "            loss = 0\n",
    "            optimizer.zero_grad()\n",
    "            frames = torch.abs(torch.fft.ifftn(fspace.squeeze())).to(DTYPE)\n",
    "            outputs = model(preprocess(invariance_transforms(frames)).unsqueeze(0))[0]\n",
    "            for _ in range(dim):\n",
    "                outputs = outputs.sum(0)\n",
    "            loss -= outputs[loc].sum()\n",
    "            iterator.set_postfix({'frames loss': loss.item(), 'mean pixel value': frames.mean().item(), 'pixel std': frames.std().item()})\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(fspace, 1e-4)\n",
    "            optimizer.step()\n",
    "        frames = torch.abs(torch.fft.ifftn(fspace))\n",
    "        skvideo.io.vwrite(f\"test_{dim}_{loc}.mp4\", (frames.squeeze().permute(1,0,2,3).detach().cpu().numpy()*255).astype(np.uint8), inputdict={'-r':'16'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skvideo\n",
    "from skvideo import io\n",
    "import torchvision\n",
    "import cortex as cx\n",
    "\n",
    "experiment='vedb_ver01'\n",
    "subject_id= 'S01'\n",
    "roi = 'hMT'\n",
    "\n",
    "mask = np.load(f'/home/matthew/Data/DorsalNet_FC/fMRI_data/{subject_id}/{experiment}/mask.npy')\n",
    "fit_dir = f'/home/matthew/remote_mounts/pomcloud0/students/matthew/Projects/VWAM/regression_fits/{model_name}/{experiment}/{subject_id}'\n",
    "fit = np.load(fit_dir+'/ridge.npz')\n",
    "roi_mask = cx.get_roi_mask(subject_id, experiment, roi)[roi]\n",
    "roi_weights = fit['weights'][:,roi_mask[mask].astype(bool)]\n",
    "roi_weights = np.nanmean(roi_weights, axis=1)\n",
    "roi_weights /= abs(roi_weights.sum())\n",
    "roi_weights = torch.tensor(roi_weights).to(DEVICE).to(DTYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DorsalNet(False, 32).eval().to(DEVICE).to(DTYPE)\n",
    "model.load_state_dict(torch.load('/home/matthew/Data/DorsalNet_FC/base_models/DorsalNet/pretrained.pth'))\n",
    "model = hook_model(model, 1)\n",
    "\n",
    "preprocess = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(112),\n",
    "    # torchvision.transforms.ToTensor(),\n",
    "    # tf.Normalize(mean=[0.43216, 0.394666, 0.37645], std=[0.22803, 0.22145, 0.216989]),\n",
    "])\n",
    "\n",
    "dl = DataLoader(ImageFolder('/home/matthew/Data/DorsalNet_FC/stimuli/NaturalMovies/images/trn', transform=preprocess), batch_size=32, shuffle=False)\n",
    "\n",
    "invariance_transforms = transforms.Compose([\n",
    "    transforms.RandomCrop((512,512), padding=5),\n",
    "    # transforms.GaussianBlur(31),\n",
    "    # transforms.RandomRotation([-5,5]),\n",
    "    # transforms.RandomResizedCrop((500,500), scale=(.95,1.05), ratio=(1,1,1)),\n",
    "    transforms.RandomCrop((512,512), padding=3),\n",
    "])\n",
    "\n",
    "lr = 1e1\n",
    "\n",
    "fspace = torch.randn((1,3,32,512,512), device=DEVICE, dtype=torch.complex64).requires_grad_(True)\n",
    "optimizer = torch.optim.Adam([fspace], lr=lr)\n",
    "iterator = tqdm(range(1000))\n",
    "for i in iterator:\n",
    "    loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    frames = torch.abs(torch.fft.ifftn(fspace.squeeze())).to(DTYPE)\n",
    "    model(preprocess(invariance_transforms(frames)).unsqueeze(0));\n",
    "    all_activations = []\n",
    "    for layer_name, layer_activations in model.activations.items():\n",
    "        layer_downsampling_fn = layer_downsampling_fns[layer_name]\n",
    "        if layer_downsampling_fn is not None:\n",
    "            layer_activations = layer_downsampling_fn(layer_activations)\n",
    "        all_activations.append(layer_activations.mean(0).flatten())\n",
    "    all_activations = torch.cat(all_activations)\n",
    "    # all_activations = torch.clip(all_activations, -1, 1)\n",
    "    # loss -= all_activations@roi_weights\n",
    "    loss -= -torch.nn.functional.cosine_similarity(all_activations.unsqueeze(0), roi_weights.unsqueeze(0))\n",
    "    iterator.set_postfix({'frames loss': loss.item(), 'mean pixel value': frames.mean().item(), 'pixel std': frames.std().item()})\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(fspace, 1e-4)\n",
    "    optimizer.step()\n",
    "frames = torch.abs(torch.fft.ifftn(fspace))\n",
    "skvideo.io.vwrite(f\"test_{roi}.mp4\", (frames.squeeze().permute(1,0,2,3).detach().cpu().numpy()*255).astype(np.uint8), inputdict={'-r':'16'})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('pomlab')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "64b29729e7e2eade24d2e13d998b7091ba7e0c8e4131adf32045de59e26c20e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
