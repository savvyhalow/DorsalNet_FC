{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms as tf\n",
    "import sys\n",
    "sys.path.append('../code')\n",
    "from dorsalnet import DorsalNet, FC, interpolate_frames\n",
    "from VWAM.utils import SingleImageFolder, choose_downsampling, iterate_children, hook_model\n",
    "from tqdm import tqdm\n",
    "\n",
    "DEVICE = 'cuda:0'\n",
    "DTYPE = torch.bfloat16\n",
    "\n",
    "model = DorsalNet(False, 32).eval().to(DEVICE).to(DTYPE)\n",
    "model.load_state_dict(torch.load('/home/matthew/Data/DorsalNet_FC/base_models/DorsalNet/pretrained.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************\n",
      "model.conv1\n",
      "old_shape: torch.Size([1, 64, 32, 56, 56])\n",
      "old # activations: torch.Size([6422528])\n",
      "new_shape: torch.Size([1, 64, 4, 4, 4])\n",
      "new # activations: torch.Size([4096])\n",
      "**************\n",
      "model.s1\n",
      "old_shape: torch.Size([1, 64, 32, 28, 28])\n",
      "old # activations: torch.Size([1605632])\n",
      "new_shape: torch.Size([1, 64, 4, 4, 4])\n",
      "new # activations: torch.Size([4096])\n",
      "**************\n",
      "model.res0\n",
      "old_shape: torch.Size([1, 32, 32, 28, 28])\n",
      "old # activations: torch.Size([802816])\n",
      "new_shape: torch.Size([1, 32, 5, 5, 5])\n",
      "new # activations: torch.Size([4000])\n",
      "**************\n",
      "model.res1\n",
      "old_shape: torch.Size([1, 32, 32, 28, 28])\n",
      "old # activations: torch.Size([802816])\n",
      "new_shape: torch.Size([1, 32, 5, 5, 5])\n",
      "new # activations: torch.Size([4000])\n",
      "**************\n",
      "model.res2\n",
      "old_shape: torch.Size([1, 32, 32, 28, 28])\n",
      "old # activations: torch.Size([802816])\n",
      "new_shape: torch.Size([1, 32, 5, 5, 5])\n",
      "new # activations: torch.Size([4000])\n",
      "**************\n",
      "model.res3\n",
      "old_shape: torch.Size([1, 32, 32, 28, 28])\n",
      "old # activations: torch.Size([802816])\n",
      "new_shape: torch.Size([1, 32, 5, 5, 5])\n",
      "new # activations: torch.Size([4000])\n",
      "**************\n",
      "model.concat\n",
      "old_shape: torch.Size([1, 96, 32, 28, 28])\n",
      "old # activations: torch.Size([2408448])\n",
      "new_shape: torch.Size([1, 96, 3, 3, 3])\n",
      "new # activations: torch.Size([2592])\n"
     ]
    }
   ],
   "source": [
    "MAX_FS = 5000\n",
    "DEPTH = 1\n",
    "input_shape = (1, 3, 32, 112, 112)\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "layers_dict = iterate_children(model, depth=DEPTH)\n",
    "layers_dict = {k: v for k, v in layers_dict.items() if not 'dropout' in k}\n",
    "model = hook_model(model, layers_dict)\n",
    "model(torch.randn(input_shape).to(DEVICE).to(DTYPE))\n",
    "\n",
    "layer_downsampling_fns = {}\n",
    "for layer_name, layer_activations in model.activations.items():\n",
    "    layer_activations = layer_activations\n",
    "    print('**************')\n",
    "    print(layer_name)\n",
    "    print('old_shape:', layer_activations.shape)\n",
    "    print('old # activations:', layer_activations.flatten().shape)\n",
    "    layer_downsampling_fn = choose_downsampling(layer_activations, MAX_FS)\n",
    "    layer_downsampling_fns[layer_name] = layer_downsampling_fn\n",
    "    if layer_downsampling_fn is not None:\n",
    "        layer_activations = layer_downsampling_fns[layer_name](layer_activations)\n",
    "    print('new_shape:', layer_activations.shape)\n",
    "    print('new # activations:', layer_activations.flatten().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key1 in model.activations.keys():\n",
    "    for key2 in model.activations.keys():\n",
    "        if key1 != key2:\n",
    "            if model.activations[key1].shape == model.activations[key2].shape:\n",
    "                if np.allclose(model.activations[key1].detach().float().cpu().numpy(), model.activations[key2].detach().float().cpu().numpy()):\n",
    "                    print(key1, key2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** NaturalMovies ****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3600/3600 [18:51<00:00,  3.18it/s]\n",
      "/home/matthew/anaconda3/envs/pomlab/lib/python3.7/site-packages/ipykernel_launcher.py:62: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "100%|██████████| 270/270 [01:24<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** vedb_ver01 ****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [07:32<00:00,  2.65it/s]\n",
      "100%|██████████| 90/90 [00:32<00:00,  2.78it/s]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import torchvision\n",
    "import os\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "interpolate_frames = torchvision.transforms.Compose([\n",
    "    torchvision.ops.Permute([1,2,3,0]),\n",
    "    torchvision.transforms.Resize([input_shape[-1],input_shape[1]]),\n",
    "    torchvision.ops.Permute([0,3,1,2]),\n",
    "])\n",
    "\n",
    "preprocess = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Resize(input_shape[-1]),\n",
    "    torchvision.transforms.CenterCrop(input_shape[-1]),\n",
    "    # torchvision.transforms.Normalize(123.0, 75.0),\n",
    "])\n",
    "\n",
    "model_name='dorsalnet'\n",
    "DTYPE=torch.float32\n",
    "iter_mode = 'children'\n",
    "iter_depth = 1\n",
    "\n",
    "save_dir = f\"/home/matthew/remote_mounts/pomcloud0/students/matthew/projects/activation_maximization/DNNs/{model_name}/\"\n",
    "\n",
    "batch_sizes = {\n",
    "    # 'LHimages': 1,\n",
    "    'NaturalMovies': 30,\n",
    "    'vedb_ver01': 50,\n",
    "    'BiomotionPilot06': 48,\n",
    "}\n",
    "\n",
    "model = model.to(DTYPE)\n",
    "\n",
    "for experiment in ['NaturalMovies', 'vedb_ver01']:\n",
    "    print('****', experiment, '****')\n",
    "    images_dir = f'/home/matthew/Data/DorsalNet_FC/stimuli/{experiment}/images/trn'\n",
    "    for split in ['trn', 'val']:\n",
    "        dataloader = DataLoader(\n",
    "            SingleImageFolder(f'/home/matthew/Data/DorsalNet_FC/stimuli/{experiment}/images/{split}', transform=preprocess),\n",
    "                batch_size=batch_sizes[experiment], \n",
    "                shuffle=False)\n",
    "        activations_dict = defaultdict(list)\n",
    "        for images in tqdm(dataloader):\n",
    "            images = interpolate_frames(images).unsqueeze(0).to(DTYPE).to(DEVICE)\n",
    "            model(images);\n",
    "            layer_names = list(model.activations.keys())\n",
    "            for layer_name in layer_names:\n",
    "                layer_activations = model.activations[layer_name].detach().cpu()\n",
    "                del model.activations[layer_name]\n",
    "                layer_downsampling_fn = layer_downsampling_fns[layer_name]\n",
    "                if not isinstance(layer_downsampling_fn, type(None)):\n",
    "                    layer_activations = layer_downsampling_fn(layer_activations)\n",
    "                # if experiment != 'LHimages':\n",
    "                #     layer_activations = torch.mean(layer_activations, 0).unsqueeze(0)\n",
    "                activations_dict[layer_name].append(layer_activations.numpy())\n",
    "        activations_dict = {name: np.concatenate(outputs, 0) for name, outputs in activations_dict.items()}\n",
    "        if not os.path.exists(f'{save_dir}/activations/{experiment}'):\n",
    "            os.makedirs(f'{save_dir}/activations/{experiment}')\n",
    "        np.savez(f'{save_dir}/activations/{experiment}/{split}_activations_v2.npz', **activations_dict)\n",
    "        activations_concatenated = np.nan_to_num(np.concatenate([value.reshape(len(value), -1) for value in list(activations_dict.values())], 1).astype(np.float))\n",
    "        np.save(f'{save_dir}/activations/{experiment}/{split}_activations_v2.npy', activations_concatenated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matthew/anaconda3/envs/pomlab/lib/python3.7/site-packages/vm_tools-1.0-py3.7.egg/vm_tools/Regression/ridge.py:30: RuntimeWarning: Mean of empty slice\n",
      "  trncc_byvox = np.nanmean(pred_by_alpha, axis=-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing SVD\n",
      "Warning! HUGE MATRIX OF WEIGHTS! 1.58x recommended size!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matthew/anaconda3/envs/pomlab/lib/python3.7/site-packages/vm_tools-1.0-py3.7.egg/vm_tools/Regression/ridge.py:236: UserWarning: First column of val_fs is NOT all ones! Consider including a DC term!\n",
      "  warnings.warn('First column of val_fs is NOT all ones! Consider including a DC term!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean cc: 0.2624363\n",
      "Computing SVD\n",
      "Warning! HUGE MATRIX OF WEIGHTS! 2.68x recommended size!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matthew/anaconda3/envs/pomlab/lib/python3.7/site-packages/vm_tools-1.0-py3.7.egg/vm_tools/Stats/utils.py:106: RuntimeWarning: Mean of empty slice\n",
      "  zs = lambda x: (x-np.nanmean(x, axis=0))/np.nanstd(x, axis=0, ddof=dof)\n",
      "/home/matthew/anaconda3/envs/pomlab/lib/python3.7/site-packages/numpy/lib/nanfunctions.py:1671: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  keepdims=keepdims)\n",
      "/home/matthew/anaconda3/envs/pomlab/lib/python3.7/site-packages/vm_tools-1.0-py3.7.egg/vm_tools/Stats/utils.py:112: RuntimeWarning: invalid value encountered in true_divide\n",
      "  r = rTmp/n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean cc: 0.18957913\n",
      "Computing SVD\n",
      "Warning! HUGE MATRIX OF WEIGHTS! 1.71x recommended size!\n",
      "mean cc: 0.22632763\n",
      "Computing SVD\n",
      "Warning! HUGE MATRIX OF WEIGHTS! 1.27x recommended size!\n",
      "mean cc: 0.3321591\n",
      "Computing SVD\n",
      "Warning! HUGE MATRIX OF WEIGHTS! 1.27x recommended size!\n",
      "mean cc: 0.37056246\n",
      "Computing SVD\n",
      "Warning! HUGE MATRIX OF WEIGHTS! 1.43x recommended size!\n",
      "mean cc: 0.25449616\n",
      "Computing SVD\n",
      "Warning! HUGE MATRIX OF WEIGHTS! 1.88x recommended size!\n",
      "mean cc: 0.18786868\n",
      "Computing SVD\n",
      "Warning! HUGE MATRIX OF WEIGHTS! 1.14x recommended size!\n",
      "mean cc: 0.2419159\n",
      "Computing SVD\n",
      "Warning! HUGE MATRIX OF WEIGHTS! 1.18x recommended size!\n",
      "mean cc: 0.25973275\n",
      "Computing SVD\n",
      "Warning! HUGE MATRIX OF WEIGHTS! 1.04x recommended size!\n",
      "mean cc: 0.22743388\n",
      "Computing SVD\n",
      "Warning! HUGE MATRIX OF WEIGHTS! 1.26x recommended size!\n",
      "mean cc: 0.23946497\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import vm_tools as vmt\n",
    "import cortex as cx\n",
    "\n",
    "experiments = ['NaturalMovies', 'vedb_ver01']\n",
    "model_name='dorsalnet'\n",
    "\n",
    "for subject_id in [f'S0{i}' for i in range(9)]:\n",
    "    for experiment in experiments:\n",
    "        if os.path.exists(f'/home/matthew/Data/DorsalNet_FC/fMRI_data/{subject_id}/{experiment}/'):\n",
    "            fit_dir = f'/home/matthew/remote_mounts/pomcloud0/students/matthew/Projects/activation_maximization/regression_fits/{model_name}/{experiment}/{subject_id}'\n",
    "            # if not os.path.exists(fit_dir+'/ridge_v2.npz'):\n",
    "            if True:\n",
    "                trn_a = np.load(f'/home/matthew/remote_mounts/pomcloud0/students/matthew/Projects/activation_maximization/DNNs/{model_name}/activations/{experiment}/trn_activations_v2.npy')\n",
    "                val_a = np.load(f'/home/matthew/remote_mounts/pomcloud0/students/matthew/Projects/activation_maximization/DNNs/{model_name}/activations/{experiment}/val_activations_v2.npy')\n",
    "\n",
    "                if experiment=='NaturalMovies' and subject_id=='S01':\n",
    "                    trn_a = trn_a[:2400]\n",
    "                    val_a = val_a[:180]\n",
    "\n",
    "                if experiment != 'LHimages':\n",
    "                    trn_a = vmt.utils.add_lags(trn_a)\n",
    "                    val_a = vmt.utils.add_lags(val_a)\n",
    "                \n",
    "                if not os.path.exists(fit_dir):\n",
    "                    os.makedirs(fit_dir)\n",
    "                save_path = fit_dir+f\"/ridge_v2.npz\"\n",
    "                trn_brain = np.load(f'/home/matthew/Data/DorsalNet_FC/fMRI_data/{subject_id}/{experiment}/trn.npy')\n",
    "                val_brain = np.load(f'/home/matthew/Data/DorsalNet_FC/fMRI_data/{subject_id}/{experiment}/val_rpts.npy').mean(0)\n",
    "                fit = vmt.Regression.ridge_cv(trn_fs=trn_a, trn_data=trn_brain,\n",
    "                                                    val_fs=val_a, val_data=val_brain,\n",
    "                                                    alphas = list(np.logspace(0,20,20)), ## default range is much too low\n",
    "                                                    select_by='individual_voxel_r2',\n",
    "                                                    do_re_zscore_fs=False, do_re_zscore_data=False, is_verbose=False,\n",
    "                                                    chunk_sz=100000,\n",
    "                                                    )\n",
    "                if experiment != 'LHimages':\n",
    "                    fit['weights_lagged'] = fit['weights'].copy()\n",
    "                    fit['last_two_lags_mean'] = np.nanmean([fit['weights_lagged'][len(fit['weights_lagged'])//3:-len(fit['weights_lagged'])//3], fit['weights_lagged'][-len(fit['weights_lagged'])//3:]], axis=0)\n",
    "                    fit['weights'] = vmt.utils.avg_wts(fit['weights'].T, skipfirst=False).T\n",
    "                np.savez(save_path, **fit)\n",
    "            fit = np.load(save_path)\n",
    "            print('mean cc:', np.nanmean(fit['cc']))\n",
    "            mask = np.load(f'/home/matthew/Data/DorsalNet_FC/fMRI_data/{subject_id}/{experiment}/mask.npy')\n",
    "            # cx.webshow(cx.Volume(fit['cc'], subject=subject_id, xfmname=experiment, mask=mask, vmin=0, vmax=1, cmap='afmhot'), title=f\"{subject_id} {experiment} ccs\", with_curvature=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation maximization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Just network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skvideo\n",
    "from skvideo import io\n",
    "import torchvision\n",
    "\n",
    "model = DorsalNet(False, 32).eval().to(DEVICE).to(DTYPE)\n",
    "model.load_state_dict(torch.load('/home/matthew/Data/DorsalNet_FC/base_models/DorsalNet/pretrained.pth'))\n",
    "\n",
    "preprocess = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(112),\n",
    "    # torchvision.transforms.ToTensor(),\n",
    "    # tf.Normalize(mean=[0.43216, 0.394666, 0.37645], std=[0.22803, 0.22145, 0.216989]),\n",
    "])\n",
    "\n",
    "dl = DataLoader(ImageFolder('/home/matthew/Data/DorsalNet_FC/stimuli/NaturalMovies/images/trn', transform=preprocess), batch_size=32, shuffle=False)\n",
    "\n",
    "invariance_transforms = transforms.Compose([\n",
    "    transforms.RandomCrop((512,512), padding=5),\n",
    "    # transforms.GaussianBlur(31),\n",
    "    # transforms.RandomRotation([-5,5]),\n",
    "    # transforms.RandomResizedCrop((500,500), scale=(.95,1.05), ratio=(1,1,1)),\n",
    "    transforms.RandomCrop((512,512), padding=3),\n",
    "])\n",
    "\n",
    "lr = 1e2\n",
    "\n",
    "for dim in range(4):\n",
    "    for loc in range(0,28,8):\n",
    "        fspace = torch.randn((1,3,32,512,512), device=DEVICE, dtype=torch.complex64).requires_grad_(True)\n",
    "        optimizer = torch.optim.Adam([fspace], lr=lr)\n",
    "        iterator = tqdm(range(100))\n",
    "        for i in iterator:\n",
    "            loss = 0\n",
    "            optimizer.zero_grad()\n",
    "            frames = torch.abs(torch.fft.ifftn(fspace.squeeze())).to(DTYPE)\n",
    "            outputs = model(preprocess(invariance_transforms(frames)).unsqueeze(0))[0]\n",
    "            for _ in range(dim):\n",
    "                outputs = outputs.sum(0)\n",
    "            loss -= outputs[loc].sum()\n",
    "            iterator.set_postfix({'frames loss': loss.item(), 'mean pixel value': frames.mean().item(), 'pixel std': frames.std().item()})\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(fspace, 1e-4)\n",
    "            optimizer.step()\n",
    "        frames = torch.abs(torch.fft.ifftn(fspace))\n",
    "        skvideo.io.vwrite(f\"test_{dim}_{loc}.mp4\", (frames.squeeze().permute(1,0,2,3).detach().cpu().numpy()*255).astype(np.uint8), inputdict={'-r':'16'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skvideo\n",
    "from skvideo import io\n",
    "import torchvision\n",
    "import cortex as cx\n",
    "\n",
    "experiment='vedb_ver01'\n",
    "subject_id= 'S01'\n",
    "roi = 'hMT'\n",
    "\n",
    "mask = np.load(f'/home/matthew/Data/DorsalNet_FC/fMRI_data/{subject_id}/{experiment}/mask.npy')\n",
    "fit_dir = f'/home/matthew/remote_mounts/pomcloud0/students/matthew/Projects/activation_maximization/regression_fits/{model_name}/{experiment}/{subject_id}'\n",
    "fit = np.load(fit_dir+'/ridge.npz')\n",
    "roi_mask = cx.get_roi_mask(subject_id, experiment, roi)[roi]\n",
    "roi_weights = fit['weights'][:,roi_mask[mask].astype(bool)]\n",
    "roi_weights = np.nanmean(roi_weights, axis=1)\n",
    "roi_weights /= abs(roi_weights.sum())\n",
    "roi_weights = torch.tensor(roi_weights).to(DEVICE).to(DTYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DorsalNet(False, 32).eval().to(DEVICE).to(DTYPE)\n",
    "model.load_state_dict(torch.load('/home/matthew/Data/DorsalNet_FC/base_models/DorsalNet/pretrained.pth'))\n",
    "model = hook_model(model, 1)\n",
    "\n",
    "preprocess = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(112),\n",
    "    # torchvision.transforms.ToTensor(),\n",
    "    # tf.Normalize(mean=[0.43216, 0.394666, 0.37645], std=[0.22803, 0.22145, 0.216989]),\n",
    "])\n",
    "\n",
    "dl = DataLoader(ImageFolder('/home/matthew/Data/DorsalNet_FC/stimuli/NaturalMovies/images/trn', transform=preprocess), batch_size=32, shuffle=False)\n",
    "\n",
    "invariance_transforms = transforms.Compose([\n",
    "    transforms.RandomCrop((512,512), padding=5),\n",
    "    # transforms.GaussianBlur(31),\n",
    "    # transforms.RandomRotation([-5,5]),\n",
    "    # transforms.RandomResizedCrop((500,500), scale=(.95,1.05), ratio=(1,1,1)),\n",
    "    transforms.RandomCrop((512,512), padding=3),\n",
    "])\n",
    "\n",
    "lr = 1e1\n",
    "\n",
    "fspace = torch.randn((1,3,32,512,512), device=DEVICE, dtype=torch.complex64).requires_grad_(True)\n",
    "optimizer = torch.optim.Adam([fspace], lr=lr)\n",
    "iterator = tqdm(range(1000))\n",
    "for i in iterator:\n",
    "    loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    frames = torch.abs(torch.fft.ifftn(fspace.squeeze())).to(DTYPE)\n",
    "    model(preprocess(invariance_transforms(frames)).unsqueeze(0));\n",
    "    all_activations = []\n",
    "    for layer_name, layer_activations in model.activations.items():\n",
    "        layer_downsampling_fn = layer_downsampling_fns[layer_name]\n",
    "        if layer_downsampling_fn is not None:\n",
    "            layer_activations = layer_downsampling_fn(layer_activations)\n",
    "        all_activations.append(layer_activations.mean(0).flatten())\n",
    "    all_activations = torch.cat(all_activations)\n",
    "    # all_activations = torch.clip(all_activations, -1, 1)\n",
    "    # loss -= all_activations@roi_weights\n",
    "    loss -= -torch.nn.functional.cosine_similarity(all_activations.unsqueeze(0), roi_weights.unsqueeze(0))\n",
    "    iterator.set_postfix({'frames loss': loss.item(), 'mean pixel value': frames.mean().item(), 'pixel std': frames.std().item()})\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(fspace, 1e-4)\n",
    "    optimizer.step()\n",
    "frames = torch.abs(torch.fft.ifftn(fspace))\n",
    "skvideo.io.vwrite(f\"test_{roi}.mp4\", (frames.squeeze().permute(1,0,2,3).detach().cpu().numpy()*255).astype(np.uint8), inputdict={'-r':'16'})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('pomlab')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "64b29729e7e2eade24d2e13d998b7091ba7e0c8e4131adf32045de59e26c20e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
