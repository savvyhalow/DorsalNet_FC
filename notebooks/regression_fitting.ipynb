{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/matthew/Code/DorsalNet_FC/code\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd ../code\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from dorsalnet import DorsalNet\n",
    "\n",
    "DEVICE = 'cuda:0'\n",
    "DTYPE = torch.float\n",
    "\n",
    "model = DorsalNet(False, 32).eval().to(DEVICE).to(DTYPE)\n",
    "model.load_state_dict(torch.load('/home/matthew/Data/DorsalNet_FC/base_models/DorsalNet/pretrained.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from functools import partial\n",
    "from collections import defaultdict\n",
    "\n",
    "def iterate_children(child, parent_name='model', depth=1):\n",
    "    if depth > 1:\n",
    "        children_list = []\n",
    "        for name, grandchild in child.named_children():\n",
    "            children_list += iterate_children(grandchild, parent_name+'.'+name, depth-1)\n",
    "        return children_list\n",
    "    else:\n",
    "        return {(parent_name+'.'+name, module) for name, module in child.named_children()}\n",
    "\n",
    "def store_activations(activations_dict, layer_name, module, input, output):\n",
    "    activations_dict[layer_name] = output\n",
    "\n",
    "def hook_model(model, depth):\n",
    "    model.activations = defaultdict(list)\n",
    "    for layer_name, child in iterate_children(model, depth=depth):\n",
    "        child.register_forward_hook(partial(store_activations, model.activations, layer_name))\n",
    "    return model\n",
    "\n",
    "def choose_downsampling(activations, max_fs):\n",
    "    if activations.ndim == 5:\n",
    "        activations = activations[0:1]\n",
    "        test_range = activations.shape[-1]\n",
    "        numels = np.zeros((test_range+1, test_range))\n",
    "        pbar = tqdm(range(sum(range(test_range+1))))\n",
    "        for k in range(1,test_range+1):\n",
    "            for s in range(1,k+1):\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix_str(f\"testing size {k}, stride {s}\")\n",
    "                n = (activations.shape[-1] - k) / s\n",
    "                if n != int(n):\n",
    "                    continue\n",
    "                else:\n",
    "                    pooled = torch.nn.functional.max_pool3d(activations, kernel_size=(2,k,k), stride=s)\n",
    "                    if pooled.shape[-1] > 1 and pooled.numel() <= max_fs:\n",
    "                        numels[k,s] = pooled.numel()\n",
    "                    else:\n",
    "                        continue\n",
    "        best_k, best_s = np.unravel_index(np.argmax(numels, axis=None), numels.shape)\n",
    "        if (best_k, best_s) == (0,0):\n",
    "            return None\n",
    "        else:\n",
    "            return torch.nn.MaxPool3d(kernel_size=(2, best_k, best_k), stride=best_s)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "interpolate_frames = torchvision.transforms.Compose([\n",
    "    torchvision.ops.Permute([1,2,3,0]),\n",
    "    torchvision.transforms.Resize([112,32]),\n",
    "    torchvision.ops.Permute([0,3,1,2]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************\n",
      "model.conv1\n",
      "old_shape: torch.Size([6422528])\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013254404067993164,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1596,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "119c000cabe649bd92ba698075d26c90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1596 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_shape: torch.Size([1280])\n",
      "**************\n",
      "model.s1\n",
      "old_shape: torch.Size([1605632])\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.016849756240844727,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 406,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "036fa23529f049f28786736946b68ad5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/406 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_shape: torch.Size([1280])\n",
      "**************\n",
      "model.res0\n",
      "old_shape: torch.Size([802816])\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.019980907440185547,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 406,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3619276dd462478c922f480ce91ffe26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/406 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_shape: torch.Size([1440])\n",
      "**************\n",
      "model.res1\n",
      "old_shape: torch.Size([802816])\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0194547176361084,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 406,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96afb50311904ef1b9118a7822c5c491",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/406 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_shape: torch.Size([1440])\n",
      "**************\n",
      "model.res2\n",
      "old_shape: torch.Size([802816])\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01243734359741211,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 406,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bb8730f027f459e8f7d5a344b8ddb80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/406 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_shape: torch.Size([1440])\n",
      "**************\n",
      "model.res3\n",
      "old_shape: torch.Size([802816])\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013966083526611328,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 406,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e924209afaf44ebba123b22c33ca585e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/406 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_shape: torch.Size([1440])\n",
      "**************\n",
      "model.concat\n",
      "old_shape: torch.Size([2408448])\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.012366771697998047,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 406,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48895a3e93704e078ea910bdc449ed75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/406 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_shape: torch.Size([1152])\n",
      "**************\n",
      "model.dropout\n",
      "old_shape: torch.Size([802816])\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.021704673767089844,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 406,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6bacfecc86a4502902767c30bcdf8c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/406 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_shape: torch.Size([1440])\n"
     ]
    }
   ],
   "source": [
    "MAX_FS = 1500\n",
    "    \n",
    "model = hook_model(model, 1)\n",
    "model(torch.randn((1, 3, 32, 112, 112)).to(DEVICE).to(DTYPE))\n",
    "\n",
    "layer_downsampling_fns = {}\n",
    "for layer_name, layer_activations in model.activations.items():\n",
    "    print('**************')\n",
    "    print(layer_name)\n",
    "    print('old_shape:', layer_activations.flatten().shape)\n",
    "    layer_downsampling_fn = choose_downsampling(layer_activations, MAX_FS)\n",
    "    layer_downsampling_fns[layer_name] = layer_downsampling_fn\n",
    "    if layer_downsampling_fn is not None:\n",
    "        layer_activations = layer_downsampling_fns[layer_name](layer_activations)\n",
    "    print('new_shape:', layer_activations.flatten().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import dill as pickle\n",
    "import os\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models import inception_v3 as model_init\n",
    "from torchvision.models import Inception_V3_Weights as model_weights\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "model_name='dorsalnet'\n",
    "DTYPE=torch.float32\n",
    "iter_mode = 'children'\n",
    "iter_depth = 1\n",
    "\n",
    "save_dir = f\"/home/matthew/remote_mounts/pomcloud0/students/matthew/projects/VWAM/DNNs/{model_name}/\"\n",
    "\n",
    "batch_sizes = {\n",
    "    'LHimages': 1,\n",
    "    'NaturalMovies': 30,\n",
    "    'vedb_ver01': 50,\n",
    "    'BiomotionPilot06': 48,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess = torchvision.transforms.Compose([\n",
    "#     torchvision.transforms.Resize(112),\n",
    "#     torchvision.transforms.ToTensor(),\n",
    "#     # tf.Normalize(mean=[0.43216, 0.394666, 0.37645], std=[0.22803, 0.22145, 0.216989]),\n",
    "# ])\n",
    "\n",
    "# for experiment in ['LHimages', 'NaturalMovies', 'vedb_ver01'][2:]:\n",
    "#     print('****', experiment, '****')\n",
    "#     images_dir = f'/hdd01/stimuli/{experiment}/ImageFolder'\n",
    "#     for split in ['trn', 'val']:\n",
    "#         dataset = datasets.ImageFolder(images_dir+'/'+split, transform=preprocess)\n",
    "#         dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_sizes[experiment], shuffle=False)\n",
    "#         activations_dict = defaultdict(list)\n",
    "#         for images, labels in tqdm(dataloader):\n",
    "#             images = interpolate_frames(images).unsqueeze(0)\n",
    "#             model(images.to(DEVICE));\n",
    "#             layer_names = list(model.activations.keys())\n",
    "#             for layer_name in layer_names:\n",
    "#                 layer_activations = model.activations[layer_name].detach().cpu()\n",
    "#                 del model.activations[layer_name]\n",
    "#                 layer_downsampling_fn = layer_downsampling_fns[layer_name]\n",
    "#                 if not isinstance(layer_downsampling_fn, type(None)):\n",
    "#                     layer_activations = layer_downsampling_fn(layer_activations)\n",
    "#                 # if experiment != 'LHimages':\n",
    "#                 #     layer_activations = torch.mean(layer_activations, 0).unsqueeze(0)\n",
    "#                 activations_dict[layer_name].append(layer_activations.numpy())\n",
    "#         activations_dict = {name: np.concatenate(outputs, 0) for name, outputs in activations_dict.items()}\n",
    "#         if not os.path.exists(f'{save_dir}/activations/{experiment}'):\n",
    "#             os.makedirs(f'{save_dir}/activations/{experiment}')\n",
    "#         np.savez(f'{save_dir}/activations/{experiment}/{split}_activations.npz', **activations_dict)\n",
    "#         activations_concatenated = np.nan_to_num(np.concatenate([value.reshape(len(value), -1) for value in list(activations_dict.values())], 1).astype(np.float))\n",
    "#         np.save(f'{save_dir}/activations/{experiment}/{split}_activations.npy', activations_concatenated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean cc: 0.27363303\n",
      "Started server on port 24215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tornado.access:200 GET /mixer.html (127.0.0.1) 47.46ms\n",
      "INFO:tornado.access:200 GET /mixer.html (127.0.0.1) 47.46ms\n",
      "INFO:tornado.access:200 GET /resources/css/jquery-ui.min.css (127.0.0.1) 1.86ms\n",
      "INFO:tornado.access:200 GET /resources/css/jquery-ui.min.css (127.0.0.1) 1.86ms\n",
      "INFO:tornado.access:200 GET /resources/css/w2ui-1.4.2.min.css (127.0.0.1) 2.49ms\n",
      "INFO:tornado.access:200 GET /resources/css/w2ui-1.4.2.min.css (127.0.0.1) 2.49ms\n",
      "INFO:tornado.access:200 GET /resources/css/select2-4.0.3.min.css (127.0.0.1) 3.18ms\n",
      "INFO:tornado.access:200 GET /resources/css/select2-4.0.3.min.css (127.0.0.1) 3.18ms\n",
      "INFO:tornado.access:200 GET /resources/js/jquery-2.1.1.min.js (127.0.0.1) 7.92ms\n",
      "INFO:tornado.access:200 GET /resources/js/jquery-2.1.1.min.js (127.0.0.1) 7.92ms\n",
      "INFO:tornado.access:200 GET /resources/js/jquery-ui.min.js (127.0.0.1) 18.35ms\n",
      "INFO:tornado.access:200 GET /resources/js/jquery-ui.min.js (127.0.0.1) 18.35ms\n",
      "INFO:tornado.access:200 GET /resources/js/jquery.ddslick.min.js (127.0.0.1) 20.18ms\n",
      "INFO:tornado.access:200 GET /resources/js/jquery.ddslick.min.js (127.0.0.1) 20.18ms\n",
      "INFO:tornado.access:200 GET /resources/css/mriview.css (127.0.0.1) 2.06ms\n",
      "INFO:tornado.access:200 GET /resources/css/mriview.css (127.0.0.1) 2.06ms\n",
      "INFO:tornado.access:200 GET /resources/css/jsplot.css (127.0.0.1) 4.83ms\n",
      "INFO:tornado.access:200 GET /resources/css/jsplot.css (127.0.0.1) 4.83ms\n",
      "INFO:tornado.access:200 GET /resources/js/dat.gui.min.js (127.0.0.1) 4.41ms\n",
      "INFO:tornado.access:200 GET /resources/js/dat.gui.min.js (127.0.0.1) 4.41ms\n",
      "INFO:tornado.access:200 GET /resources/js/select2-4.0.3.min.js (127.0.0.1) 7.76ms\n",
      "INFO:tornado.access:200 GET /resources/js/select2-4.0.3.min.js (127.0.0.1) 7.76ms\n",
      "INFO:tornado.access:200 GET /resources/js/three.js (127.0.0.1) 34.08ms\n",
      "INFO:tornado.access:200 GET /resources/js/three.js (127.0.0.1) 34.08ms\n",
      "INFO:tornado.access:200 GET /resources/js/OculusRiftEffect.js (127.0.0.1) 35.92ms\n",
      "INFO:tornado.access:200 GET /resources/js/OculusRiftEffect.js (127.0.0.1) 35.92ms\n",
      "INFO:tornado.access:200 GET /resources/js/ctm/lzma.js (127.0.0.1) 2.99ms\n",
      "INFO:tornado.access:200 GET /resources/js/ctm/lzma.js (127.0.0.1) 2.99ms\n",
      "INFO:tornado.access:200 GET /resources/js/ctm/CTMLoader.js (127.0.0.1) 6.30ms\n",
      "INFO:tornado.access:200 GET /resources/js/ctm/CTMLoader.js (127.0.0.1) 6.30ms\n",
      "INFO:tornado.access:200 GET /resources/js/ctm/ctm.js (127.0.0.1) 5.82ms\n",
      "INFO:tornado.access:200 GET /resources/js/ctm/ctm.js (127.0.0.1) 5.82ms\n",
      "INFO:tornado.access:200 GET /resources/js/svg_todataurl.js (127.0.0.1) 7.51ms\n",
      "INFO:tornado.access:200 GET /resources/js/svg_todataurl.js (127.0.0.1) 7.51ms\n",
      "INFO:tornado.access:200 GET /resources/js/datamodel.js (127.0.0.1) 9.35ms\n",
      "INFO:tornado.access:200 GET /resources/js/datamodel.js (127.0.0.1) 9.35ms\n",
      "INFO:tornado.access:200 GET /resources/js/dataset.js (127.0.0.1) 11.55ms\n",
      "INFO:tornado.access:200 GET /resources/js/dataset.js (127.0.0.1) 11.55ms\n",
      "INFO:tornado.access:200 GET /resources/js/sliceplane.js (127.0.0.1) 2.40ms\n",
      "INFO:tornado.access:200 GET /resources/js/sliceplane.js (127.0.0.1) 2.40ms\n",
      "INFO:tornado.access:200 GET /resources/js/svgoverlay.js (127.0.0.1) 5.50ms\n",
      "INFO:tornado.access:200 GET /resources/js/svgoverlay.js (127.0.0.1) 5.50ms\n",
      "INFO:tornado.access:200 GET /resources/js/shaderlib.js (127.0.0.1) 5.87ms\n",
      "INFO:tornado.access:200 GET /resources/js/shaderlib.js (127.0.0.1) 5.87ms\n",
      "INFO:tornado.access:200 GET /resources/js/movement.js (127.0.0.1) 7.85ms\n",
      "INFO:tornado.access:200 GET /resources/js/movement.js (127.0.0.1) 7.85ms\n",
      "INFO:tornado.access:200 GET /resources/js/menu.js (127.0.0.1) 9.13ms\n",
      "INFO:tornado.access:200 GET /resources/js/menu.js (127.0.0.1) 9.13ms\n",
      "INFO:tornado.access:200 GET /resources/js/kdTree-min.js (127.0.0.1) 10.86ms\n",
      "INFO:tornado.access:200 GET /resources/js/kdTree-min.js (127.0.0.1) 10.86ms\n",
      "INFO:tornado.access:200 GET /resources/js/facepick.js (127.0.0.1) 1.73ms\n",
      "INFO:tornado.access:200 GET /resources/js/facepick.js (127.0.0.1) 1.73ms\n",
      "INFO:tornado.access:200 GET /resources/js/w2ui-1.4.2.min.js (127.0.0.1) 14.83ms\n",
      "INFO:tornado.access:200 GET /resources/js/w2ui-1.4.2.min.js (127.0.0.1) 14.83ms\n",
      "INFO:tornado.access:200 GET /resources/js/figure.js (127.0.0.1) 14.80ms\n",
      "INFO:tornado.access:200 GET /resources/js/figure.js (127.0.0.1) 14.80ms\n",
      "INFO:tornado.access:200 GET /resources/js/axes3d.js (127.0.0.1) 16.50ms\n",
      "INFO:tornado.access:200 GET /resources/js/axes3d.js (127.0.0.1) 16.50ms\n",
      "INFO:tornado.access:200 GET /resources/js/mriview_utils.js (127.0.0.1) 18.29ms\n",
      "INFO:tornado.access:200 GET /resources/js/mriview_utils.js (127.0.0.1) 18.29ms\n",
      "INFO:tornado.access:200 GET /resources/js/mriview_surface.js (127.0.0.1) 20.97ms\n",
      "INFO:tornado.access:200 GET /resources/js/mriview_surface.js (127.0.0.1) 20.97ms\n",
      "INFO:tornado.access:200 GET /resources/js/mriview.js (127.0.0.1) 2.58ms\n",
      "INFO:tornado.access:200 GET /resources/js/mriview.js (127.0.0.1) 2.58ms\n",
      "INFO:tornado.access:200 GET /resources/js/leap-0.6.4.js (127.0.0.1) 14.31ms\n",
      "INFO:tornado.access:200 GET /resources/js/leap-0.6.4.js (127.0.0.1) 14.31ms\n",
      "INFO:tornado.access:200 GET /resources/js/leap.js (127.0.0.1) 12.64ms\n",
      "INFO:tornado.access:200 GET /resources/js/leap.js (127.0.0.1) 12.64ms\n",
      "INFO:tornado.access:200 GET /resources/js/python_interface.js (127.0.0.1) 14.49ms\n",
      "INFO:tornado.access:200 GET /resources/js/python_interface.js (127.0.0.1) 14.49ms\n",
      "INFO:tornado.access:200 GET /resources/css/images/loading.gif (127.0.0.1) 1.45ms\n",
      "INFO:tornado.access:200 GET /resources/css/images/loading.gif (127.0.0.1) 1.45ms\n",
      "INFO:tornado.access:200 GET /ctm/S01/ (127.0.0.1) 0.78ms\n",
      "INFO:tornado.access:200 GET /ctm/S01/ (127.0.0.1) 0.78ms\n",
      "INFO:tornado.access:101 GET /wsconnect/ (127.0.0.1) 0.90ms\n",
      "INFO:tornado.access:101 GET /wsconnect/ (127.0.0.1) 0.90ms\n",
      "INFO:tornado.access:200 GET /data/__f75fbb888190d874/0/ (127.0.0.1) 0.83ms\n",
      "INFO:tornado.access:200 GET /data/__f75fbb888190d874/0/ (127.0.0.1) 0.83ms\n",
      "INFO:tornado.access:200 GET /ctm/S01/S01_[inflated]_mg2_9_v3.ctm (127.0.0.1) 18.16ms\n",
      "INFO:tornado.access:200 GET /ctm/S01/S01_[inflated]_mg2_9_v3.ctm (127.0.0.1) 18.16ms\n",
      "INFO:tornado.access:200 GET /favicon.ico (127.0.0.1) 1.42ms\n",
      "INFO:tornado.access:200 GET /favicon.ico (127.0.0.1) 1.42ms\n",
      "INFO:tornado.access:200 GET /resources/js/ctm/CTMWorker.js (127.0.0.1) 1.20ms\n",
      "INFO:tornado.access:200 GET /resources/js/ctm/CTMWorker.js (127.0.0.1) 1.20ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tornado.access:200 GET /resources/js/facepick_worker.js (127.0.0.1) 1.52ms\n",
      "INFO:tornado.access:200 GET /resources/js/facepick_worker.js (127.0.0.1) 1.52ms\n",
      "INFO:tornado.access:200 GET /ctm/S01/S01_[inflated]_mg2_9_v3.svg (127.0.0.1) 16.95ms\n",
      "INFO:tornado.access:200 GET /ctm/S01/S01_[inflated]_mg2_9_v3.svg (127.0.0.1) 16.95ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping server\n",
      "Stopping server\n",
      "Stopping server\n"
     ]
    }
   ],
   "source": [
    "import vm_tools as vmt\n",
    "import cortex as cx\n",
    "\n",
    "experiments = ['NaturalMovies', 'vedb_ver01']\n",
    "\n",
    "for subject_id in [f'S0{i}' for i in range(9)]:\n",
    "    for experiment in experiments:\n",
    "        if os.path.exists(f'/home/matthew/Data/DorsalNet_FC/fMRI_data/{subject_id}/{experiment}/'):\n",
    "            fit_dir = f'/home/matthew/remote_mounts/pomcloud0/students/matthew/Projects/VWAM/regression_fits/{model_name}/{experiment}/{subject_id}'\n",
    "            if os.path.exists(fit_dir+'/ridge.npz'):\n",
    "                fit = np.load(fit_dir+'/ridge.npz')\n",
    "            else:\n",
    "                trn_a = np.load(f'/home/matthew/remote_mounts/pomcloud0/students/matthew/Projects/VWAM/DNNs/{model_name}/activations/{experiment}/trn_activations.npy')\n",
    "                val_a = np.load(f'/home/matthew/remote_mounts/pomcloud0/students/matthew/Projects/VWAM/DNNs/{model_name}/activations/{experiment}/val_activations.npy')\n",
    "\n",
    "                if experiment=='NaturalMovies' and subject_id=='S01':\n",
    "                    trn_a = trn_a[:2400]\n",
    "                    val_a = val_a[:180]\n",
    "\n",
    "                if experiment != 'LHimages':\n",
    "                    trn_a = vmt.utils.add_lags(trn_a)\n",
    "                    val_a = vmt.utils.add_lags(val_a)\n",
    "                \n",
    "                if not os.path.exists(fit_dir):\n",
    "                    os.makedirs(fit_dir)\n",
    "                save_path = fit_dir+f\"/ridge.npz\"\n",
    "                trn_brain = np.load(f'/home/matthew/Data/DorsalNet_FC/fMRI_data/{subject_id}/{experiment}/trn.npy')\n",
    "                val_brain = np.load(f'/home/matthew/Data/DorsalNet_FC/fMRI_data/{subject_id}/{experiment}/val_rpts.npy').mean(0)\n",
    "                fit = vmt.Regression.ridge_cv(trn_fs=trn_a, trn_data=trn_brain,\n",
    "                                                    val_fs=val_a, val_data=val_brain,\n",
    "                                                    alphas = list(np.logspace(0,20,20)), ## default range is much too low\n",
    "                                                    select_by='individual_voxel_r2',\n",
    "                                                    do_re_zscore_fs=False, do_re_zscore_data=False, is_verbose=False,\n",
    "                                                    chunk_sz=100000,\n",
    "                                                    )\n",
    "                if experiment != 'LHimages':\n",
    "                    fit['weights_lagged'] = fit['weights'].copy()\n",
    "                    fit['last_two_lags_mean'] = np.nanmean([fit['weights_lagged'][len(fit['weights_lagged'])//3:-len(fit['weights_lagged'])//3], fit['weights_lagged'][-len(fit['weights_lagged'])//3:]], axis=0)\n",
    "                    fit['weights'] = vmt.utils.avg_wts(fit['weights'].T, skipfirst=False).T\n",
    "                np.savez(save_path, **fit)\n",
    "            print('mean cc:', np.nanmean(fit['cc']))\n",
    "            mask = np.load(f'/home/matthew/Data/DorsalNet_FC/fMRI_data/{subject_id}/{experiment}/mask.npy')\n",
    "            cx.webshow(cx.Volume(fit['cc'], subject=subject_id, xfmname=experiment, mask=mask, vmin=0, vmax=1, cmap='afmhot'), title=f\"{subject_id} {experiment} ccs\", with_curvature=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation maximization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:04<00:00, 20.35it/s, frames loss=-3.13e+4, mean pixel value=0.086, pixel std=0.114] \n",
      "100%|██████████| 100/100 [00:04<00:00, 20.38it/s, frames loss=-2.57e+4, mean pixel value=0.0727, pixel std=0.124]\n",
      "100%|██████████| 100/100 [00:04<00:00, 20.49it/s, frames loss=0, mean pixel value=0.000177, pixel std=9.23e-5]\n",
      "100%|██████████| 100/100 [00:04<00:00, 20.52it/s, frames loss=-9.43e+4, mean pixel value=0.0729, pixel std=0.125]\n",
      "100%|██████████| 100/100 [00:04<00:00, 20.44it/s, frames loss=-1.42e+4, mean pixel value=0.0419, pixel std=0.136]\n",
      "100%|██████████| 100/100 [00:04<00:00, 20.38it/s, frames loss=-2.16e+4, mean pixel value=0.054, pixel std=0.132] \n",
      "100%|██████████| 100/100 [00:04<00:00, 20.43it/s, frames loss=-1.82e+4, mean pixel value=0.054, pixel std=0.133] \n",
      "100%|██████████| 100/100 [00:04<00:00, 20.34it/s, frames loss=-1.56e+4, mean pixel value=0.0546, pixel std=0.132]\n",
      "100%|██████████| 100/100 [00:04<00:00, 20.44it/s, frames loss=-3.26e+4, mean pixel value=0.0449, pixel std=0.13] \n",
      "100%|██████████| 100/100 [00:04<00:00, 20.30it/s, frames loss=-3.85e+4, mean pixel value=0.0565, pixel std=0.129]\n",
      "100%|██████████| 100/100 [00:04<00:00, 20.40it/s, frames loss=-3.71e+4, mean pixel value=0.0568, pixel std=0.13] \n",
      "100%|██████████| 100/100 [00:04<00:00, 20.37it/s, frames loss=-6.68e+4, mean pixel value=0.0526, pixel std=0.132]\n",
      "100%|██████████| 100/100 [00:04<00:00, 20.42it/s, frames loss=-2.53e+4, mean pixel value=0.0451, pixel std=0.139]\n",
      "100%|██████████| 100/100 [00:04<00:00, 20.47it/s, frames loss=-2.47e+4, mean pixel value=0.0558, pixel std=0.129]\n",
      "100%|██████████| 100/100 [00:04<00:00, 20.26it/s, frames loss=-2.45e+4, mean pixel value=0.0569, pixel std=0.128]\n",
      "100%|██████████| 100/100 [00:04<00:00, 20.22it/s, frames loss=-1.5e+4, mean pixel value=0.0557, pixel std=0.131] \n"
     ]
    }
   ],
   "source": [
    "import skvideo\n",
    "from skvideo import io\n",
    "import torchvision\n",
    "\n",
    "model = DorsalNet(False, 32).eval().to(DEVICE).to(DTYPE)\n",
    "model.load_state_dict(torch.load('/home/matthew/Data/DorsalNet_FC/base_models/DorsalNet/pretrained.pth'))\n",
    "\n",
    "preprocess = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(112),\n",
    "    # torchvision.transforms.ToTensor(),\n",
    "    # tf.Normalize(mean=[0.43216, 0.394666, 0.37645], std=[0.22803, 0.22145, 0.216989]),\n",
    "])\n",
    "\n",
    "dl = DataLoader(ImageFolder('/home/matthew/Data/DorsalNet_FC/stimuli/NaturalMovies/images/trn', transform=preprocess), batch_size=32, shuffle=False)\n",
    "\n",
    "invariance_transforms = transforms.Compose([\n",
    "    transforms.RandomCrop((512,512), padding=5),\n",
    "    # transforms.GaussianBlur(31),\n",
    "    # transforms.RandomRotation([-5,5]),\n",
    "    # transforms.RandomResizedCrop((500,500), scale=(.95,1.05), ratio=(1,1,1)),\n",
    "    transforms.RandomCrop((512,512), padding=3),\n",
    "])\n",
    "\n",
    "lr = 1e2\n",
    "\n",
    "for dim in range(4):\n",
    "    for loc in range(0,28,8):\n",
    "        fspace = torch.randn((1,3,32,512,512), device=DEVICE, dtype=torch.complex64).requires_grad_(True)\n",
    "        optimizer = torch.optim.Adam([fspace], lr=lr)\n",
    "        iterator = tqdm(range(100))\n",
    "        for i in iterator:\n",
    "            loss = 0\n",
    "            optimizer.zero_grad()\n",
    "            frames = torch.abs(torch.fft.ifftn(fspace.squeeze())).to(DTYPE)\n",
    "            outputs = model(preprocess(invariance_transforms(frames)).unsqueeze(0))[0]\n",
    "            for _ in range(dim):\n",
    "                outputs = outputs.sum(0)\n",
    "            loss -= outputs[loc].sum()\n",
    "            iterator.set_postfix({'frames loss': loss.item(), 'mean pixel value': frames.mean().item(), 'pixel std': frames.std().item()})\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(fspace, 1e-4)\n",
    "            optimizer.step()\n",
    "        frames = torch.abs(torch.fft.ifftn(fspace))\n",
    "        skvideo.io.vwrite(f\"test_{dim}_{loc}.mp4\", (frames.squeeze().permute(1,0,2,3).detach().cpu().numpy()*255).astype(np.uint8), inputdict={'-r':'16'})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('pomlab')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "64b29729e7e2eade24d2e13d998b7091ba7e0c8e4131adf32045de59e26c20e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
