{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matthew/anaconda3/envs/DorsalNet_FC/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/matthew/anaconda3/envs/DorsalNet_FC/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms as tf\n",
    "import sys\n",
    "from dorsalnet import DorsalNet, FC, interpolate_frames\n",
    "sys.path.append('../code')\n",
    "from VWAM.utils import SingleImageFolder, iterate_children, hook_model\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "DEVICE = 'cuda:0'\n",
    "DTYPE = torch.bfloat16\n",
    "MODEL_NAME = 'DorsalNet'\n",
    "OPTIMIZER = 'Adam'\n",
    "\n",
    "if MODEL_NAME == 'DorsalNet':\n",
    "    model = DorsalNet(False, 32).eval().to(DEVICE).to(DTYPE)\n",
    "    model.load_state_dict(torch.load('/home/matthew/Data/DorsalNet_FC/base_models/DorsalNet/pretrained.pth'))\n",
    "elif MODEL_NAME == 'alexnet':\n",
    "    model = torch.hub.load('pytorch/vision:v0.6.0', 'alexnet', pretrained=True).eval().to(DEVICE).to(DTYPE)\n",
    "elif MODEL_NAME == 'inception_v3':\n",
    "    model = torch.hub.load('pytorch/vision:v0.6.0', 'inception_v3', pretrained=True).eval().to(DEVICE).to(DTYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************\n",
      "model.conv1\n",
      "old_shape: torch.Size([1, 64, 32, 56, 56])\n",
      "old # activations: torch.Size([6422528])\n",
      "new_shape: torch.Size([1, 64, 4, 4, 4])\n",
      "new # activations: torch.Size([4096])\n",
      "**************\n",
      "model.s1\n",
      "old_shape: torch.Size([1, 64, 32, 28, 28])\n",
      "old # activations: torch.Size([1605632])\n",
      "new_shape: torch.Size([1, 64, 4, 4, 4])\n",
      "new # activations: torch.Size([4096])\n",
      "**************\n",
      "model.res0\n",
      "old_shape: torch.Size([1, 32, 32, 28, 28])\n",
      "old # activations: torch.Size([802816])\n",
      "new_shape: torch.Size([1, 32, 5, 5, 5])\n",
      "new # activations: torch.Size([4000])\n",
      "**************\n",
      "model.res1\n",
      "old_shape: torch.Size([1, 32, 32, 28, 28])\n",
      "old # activations: torch.Size([802816])\n",
      "new_shape: torch.Size([1, 32, 5, 5, 5])\n",
      "new # activations: torch.Size([4000])\n",
      "**************\n",
      "model.res2\n",
      "old_shape: torch.Size([1, 32, 32, 28, 28])\n",
      "old # activations: torch.Size([802816])\n",
      "new_shape: torch.Size([1, 32, 5, 5, 5])\n",
      "new # activations: torch.Size([4000])\n",
      "**************\n",
      "model.res3\n",
      "old_shape: torch.Size([1, 32, 32, 28, 28])\n",
      "old # activations: torch.Size([802816])\n",
      "new_shape: torch.Size([1, 32, 5, 5, 5])\n",
      "new # activations: torch.Size([4000])\n"
     ]
    }
   ],
   "source": [
    "MAX_FS = 5000\n",
    "DEPTH = 1\n",
    "input_size = (1, 3, 32, 112, 112)\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "# from tqdm.notebook import tqdm\n",
    "\n",
    "def choose_downsampling(activations, max_fs):\n",
    "    num_channels = activations.shape[1]\n",
    "    if activations.ndim == 4:\n",
    "        max_output_dim = int((max_fs / num_channels)**(1/2))\n",
    "        return torch.nn.AdaptiveMaxPool2d(max_output_dim)\n",
    "    elif activations.ndim == 5:\n",
    "        max_output_dim = int((max_fs / num_channels)**(1/3))\n",
    "        return torch.nn.AdaptiveMaxPool3d(max_output_dim)\n",
    "\n",
    "layers_dict = iterate_children(model, depth=DEPTH)\n",
    "layers_dict = {layer_name: ds_function for layer_name, ds_function in layers_dict.items() if 'dropout' not in layer_name and 'concat' not in layer_name}\n",
    "model = hook_model(model, layers_dict)\n",
    "model(torch.randn(input_size).to(DEVICE).to(DTYPE))\n",
    "\n",
    "layer_downsampling_fns = {}\n",
    "for layer_name, layer_activations in model.activations.items():\n",
    "    layer_activations = layer_activations\n",
    "    print('**************')\n",
    "    print(layer_name)\n",
    "    print('old_shape:', layer_activations.shape)\n",
    "    print('old # activations:', layer_activations.flatten().shape)\n",
    "    layer_downsampling_fn = choose_downsampling(layer_activations, MAX_FS)\n",
    "    layer_downsampling_fns[layer_name] = layer_downsampling_fn\n",
    "    if layer_downsampling_fn is not None:\n",
    "        layer_activations = layer_downsampling_fns[layer_name](layer_activations)\n",
    "    print('new_shape:', layer_activations.shape)\n",
    "    print('new # activations:', layer_activations.flatten().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize FC Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FC(\n",
      "  (linear): LazyLinear(in_features=0, out_features=9853, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matthew/.local/lib/python3.10/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "SUBJECT_ID = 'S00'\n",
    "trn_brain = np.load(f'/home/matthew/Data/DorsalNet_FC/fMRI_data/{SUBJECT_ID}/NaturalMovies/trn.npy')\n",
    "trn_brain = torch.tensor(np.nan_to_num(trn_brain), device=DEVICE)\n",
    "n_voxels = trn_brain.shape[1]\n",
    "\n",
    "val_brain = np.load(f'/home/matthew/Data/DorsalNet_FC/fMRI_data/{SUBJECT_ID}/NaturalMovies/val_rpts.npy')\n",
    "val_brain = torch.tensor(np.nan_to_num(val_brain).mean(0), device=DEVICE)\n",
    "\n",
    "fc = FC(n_voxels).to(DEVICE).to(DTYPE)\n",
    "print(fc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "from torch.optim import Adam\n",
    "\n",
    "def column_corr(A, B, dof=0):\n",
    "    \"\"\"Efficiently compute correlations between columns of two matrices\n",
    "    \n",
    "    Does NOT compute full correlation matrix btw `A` and `B`; returns a \n",
    "    vector of correlation coefficients. FKA ccMatrix.\"\"\"\n",
    "    zs = lambda x: (x-np.nanmean(x, axis=0))/np.nanstd(x, axis=0, ddof=dof)\n",
    "    rTmp = np.nansum(zs(A)*zs(B), axis=0)\n",
    "    n = A.shape[0]\n",
    "    # make sure not to count nans\n",
    "    nNaN = np.sum(np.logical_or(np.isnan(zs(A)), np.isnan(zs(B))), 0)\n",
    "    n = n - nNaN\n",
    "    r = rTmp/n\n",
    "    return r\n",
    "\n",
    "\n",
    "batch_sizes = {\n",
    "    'NaturalMovies': 30,\n",
    "    'vedb_ver01': 50,\n",
    "}\n",
    "\n",
    "preprocess = tf.Compose([\n",
    "    tf.Resize(112),\n",
    "    tf.ToTensor(),\n",
    "])\n",
    "\n",
    "image_augmentations = tf.Compose([\n",
    "    tf.RandomCrop(112, padding=4),\n",
    "    tf.RandomRotation(10),\n",
    "    tf.RandomCrop(112, padding=3),\n",
    "])\n",
    "\n",
    "EXPERIMENT = 'NaturalMovies'\n",
    "\n",
    "trn_dl = DataLoader(\n",
    "    SingleImageFolder(f'/home/matthew/Data/DorsalNet_FC/stimuli/{EXPERIMENT}/images/trn', transform=preprocess),\n",
    "    batch_size=batch_sizes[EXPERIMENT], \n",
    "    shuffle=False)\n",
    "\n",
    "val_dl = DataLoader(\n",
    "    SingleImageFolder(f'/home/matthew/Data/DorsalNet_FC/stimuli/{EXPERIMENT}/images/val', transform=preprocess),\n",
    "    batch_size=batch_sizes[EXPERIMENT], \n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3of11ody) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">elated-dust-28</strong> at: <a href='https://wandb.ai/mshinkle/DorsalNet_FC_Pilot/runs/3of11ody' target=\"_blank\">https://wandb.ai/mshinkle/DorsalNet_FC_Pilot/runs/3of11ody</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230427_151117-3of11ody/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3of11ody). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "991abb760c834880b44e32cd813f677c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016668893599611087, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/matthew/Code/DorsalNet_FC/notebooks/wandb/run-20230427_151124-cf7vfq6f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mshinkle/DorsalNet_FC_Pilot/runs/cf7vfq6f' target=\"_blank\">resilient-forest-29</a></strong> to <a href='https://wandb.ai/mshinkle/DorsalNet_FC_Pilot' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mshinkle/DorsalNet_FC_Pilot' target=\"_blank\">https://wandb.ai/mshinkle/DorsalNet_FC_Pilot</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mshinkle/DorsalNet_FC_Pilot/runs/cf7vfq6f' target=\"_blank\">https://wandb.ai/mshinkle/DorsalNet_FC_Pilot/runs/cf7vfq6f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "N_EPOCHS = 50\n",
    "LR_INIT = 1e-1\n",
    "\n",
    "run = wandb.init(\n",
    "    # Set the project where this run will be logged\n",
    "    project=\"DorsalNet_FC_Pilot\",\n",
    "    # Track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"model_name\": MODEL_NAME\n",
    "        \"experiment\": EXPERIMENT,\n",
    "        \"subject_id\": SUBJECT_ID,\n",
    "        \"optimizer\": OPTIMIZER,\n",
    "        \"epochs\": N_EPOCHS,\n",
    "        \"learning_rate\": LR_INIT,\n",
    "})\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "if OPTIMIZER == 'SGD':\n",
    "    optimizer = torch.optim.SGD(fc.parameters(), lr=LR_INIT)\n",
    "elif OPTIMIZER == 'Adam':\n",
    "    optimizer = torch.optim.Adam(fc.parameters(), lr=LR_INIT)\n",
    "\n",
    "def train():\n",
    "    pbar = tqdm(enumerate(trn_dl), total=len(trn_brain), desc=f\"Epoch {epoch} Training\")\n",
    "    trn_epoch_losses = []\n",
    "    for i, batch in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        batch = interpolate_frames(batch, input_size[2])\n",
    "        model.forward(image_augmentations(batch).unsqueeze(0).to(DTYPE).to(DEVICE))\n",
    "        all_activations = []\n",
    "        for layer_name, layer_activations in model.activations.items():\n",
    "            layer_downsampling_fn = layer_downsampling_fns[layer_name]\n",
    "            if layer_downsampling_fn is not None:\n",
    "                layer_activations = layer_downsampling_fn(layer_activations)\n",
    "            all_activations.append(layer_activations.mean(0).flatten())\n",
    "            model.activations[layer_name] = 0\n",
    "        fc_out = fc(torch.cat(all_activations).unsqueeze(0))\n",
    "        batch_brain = (trn_brain[min(i+2, len(trn_brain)-1)] + trn_brain[min(i+3, len(trn_brain)-1)]) / 2\n",
    "        loss = torch.square(fc_out[0]/1000 - batch_brain).sum().sqrt()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        trn_epoch_losses.append(loss.item())\n",
    "        pbar.set_postfix_str(f\"Mean Epoch Loss: {torch.mean(torch.tensor(trn_epoch_losses)).item():.2f}\")\n",
    "    return trn_epoch_losses\n",
    "\n",
    "def validate():\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(enumerate(val_dl), total=len(val_brain), desc=f\"Epoch {epoch} Validation\")\n",
    "        val_outputs = []\n",
    "        val_epoch_losses = []\n",
    "        for i, batch in pbar:\n",
    "            batch = interpolate_frames(batch, input_size[2])\n",
    "            model.forward(batch.unsqueeze(0).to(DTYPE).to(DEVICE))\n",
    "            all_activations = []\n",
    "            for layer_name, layer_activations in model.activations.items():\n",
    "                layer_downsampling_fn = layer_downsampling_fns[layer_name]\n",
    "                if layer_downsampling_fn is not None:\n",
    "                    layer_activations = layer_downsampling_fn(layer_activations)\n",
    "                all_activations.append(layer_activations.mean(0).flatten())\n",
    "                model.activations[layer_name] = 0\n",
    "            fc_out = fc(torch.cat(all_activations).unsqueeze(0))\n",
    "            batch_brain = (val_brain[min(i+2, len(val_brain)-1)] + val_brain[min(i+3, len(val_brain)-1)]) / 2\n",
    "            loss = torch.square(fc_out[0]/1000 - batch_brain).sum().sqrt()\n",
    "            val_outputs.append(fc_out.cpu().float().numpy())\n",
    "            val_epoch_losses.append(loss.item())\n",
    "            pbar.set_postfix_str(f\"Mean Epoch Loss: {torch.mean(torch.tensor(val_epoch_losses)).item():.2f}\")\n",
    "        ccs = column_corr(np.concatenate(val_outputs), val_brain.cpu().numpy())\n",
    "        print(f\"Mean Prediction Accuracy: {ccs.mean():.2f}\")\n",
    "    return val_epoch_losses, ccs\n",
    "        \n",
    "def log(epoch, trn_epoch_losses, val_epoch_losses, ccs):\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch,\n",
    "        \"trn_loss\": torch.mean(torch.tensor(trn_epoch_losses)).item(),\n",
    "        \"val_loss\": torch.mean(torch.tensor(val_epoch_losses)).item(),\n",
    "        \"val_acc\": ccs.mean(),\n",
    "    })\n",
    "\n",
    "save_dir = f'/home/matthew/Data/DorsalNet_FC/fits/{EXPERIMENT}/{SUBJECT_ID}'\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 Training:  41%|████      | 1474/3600 [08:48<12:41,  2.79it/s, Mean Epoch Loss: 81.69]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# epoch = -1\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# validate()\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(N_EPOCHS):\n\u001b[0;32m----> 4\u001b[0m     trn_epoch_losses \u001b[39m=\u001b[39m train()\n\u001b[1;32m      5\u001b[0m     val_epoch_losses, ccs \u001b[39m=\u001b[39m validate()\n\u001b[1;32m      6\u001b[0m     log(epoch, trn_epoch_losses, val_epoch_losses, ccs)\n",
      "Cell \u001b[0;32mIn[7], line 23\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m pbar \u001b[39m=\u001b[39m tqdm(\u001b[39menumerate\u001b[39m(trn_dl), total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(trn_brain), desc\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m Training\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m trn_epoch_losses \u001b[39m=\u001b[39m []\n\u001b[0;32m---> 23\u001b[0m \u001b[39mfor\u001b[39;00m i, batch \u001b[39min\u001b[39;00m pbar:\n\u001b[1;32m     24\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     25\u001b[0m     batch \u001b[39m=\u001b[39m interpolate_frames(batch, input_size[\u001b[39m2\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/DorsalNet_FC/lib/python3.10/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/DorsalNet_FC/lib/python3.10/site-packages/VWAM-0.0.1-py3.10.egg/VWAM/utils.py:19\u001b[0m, in \u001b[0;36mSingleImageFolder.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     17\u001b[0m image \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mopen(image_path)\n\u001b[1;32m     18\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 19\u001b[0m     image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(image)\n\u001b[1;32m     20\u001b[0m \u001b[39mreturn\u001b[39;00m image\n",
      "File \u001b[0;32m~/anaconda3/envs/DorsalNet_FC/lib/python3.10/site-packages/torchvision/transforms/transforms.py:94\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m     93\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m---> 94\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[1;32m     95\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/DorsalNet_FC/lib/python3.10/site-packages/torchvision/transforms/transforms.py:349\u001b[0m, in \u001b[0;36mResize.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m    342\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \u001b[39m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[39m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 349\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mresize(img, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msize, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minterpolation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mantialias)\n",
      "File \u001b[0;32m~/anaconda3/envs/DorsalNet_FC/lib/python3.10/site-packages/torchvision/transforms/functional.py:430\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    428\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    429\u001b[0m     pil_interpolation \u001b[39m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[0;32m--> 430\u001b[0m     \u001b[39mreturn\u001b[39;00m F_pil\u001b[39m.\u001b[39;49mresize(img, size\u001b[39m=\u001b[39;49msize, interpolation\u001b[39m=\u001b[39;49mpil_interpolation, max_size\u001b[39m=\u001b[39;49mmax_size)\n\u001b[1;32m    432\u001b[0m \u001b[39mreturn\u001b[39;00m F_t\u001b[39m.\u001b[39mresize(img, size\u001b[39m=\u001b[39msize, interpolation\u001b[39m=\u001b[39minterpolation\u001b[39m.\u001b[39mvalue, max_size\u001b[39m=\u001b[39mmax_size, antialias\u001b[39m=\u001b[39mantialias)\n",
      "File \u001b[0;32m~/anaconda3/envs/DorsalNet_FC/lib/python3.10/site-packages/torchvision/transforms/functional_pil.py:275\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size)\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[39mreturn\u001b[39;00m img\n\u001b[1;32m    274\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 275\u001b[0m         \u001b[39mreturn\u001b[39;00m img\u001b[39m.\u001b[39;49mresize((new_w, new_h), interpolation)\n\u001b[1;32m    276\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m     \u001b[39mif\u001b[39;00m max_size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/DorsalNet_FC/lib/python3.10/site-packages/PIL/Image.py:2156\u001b[0m, in \u001b[0;36mImage.resize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   2152\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n\u001b[1;32m   2154\u001b[0m size \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(size)\n\u001b[0;32m-> 2156\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m   2157\u001b[0m \u001b[39mif\u001b[39;00m box \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2158\u001b[0m     box \u001b[39m=\u001b[39m (\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msize\n",
      "File \u001b[0;32m~/anaconda3/envs/DorsalNet_FC/lib/python3.10/site-packages/PIL/ImageFile.py:269\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(msg)\n\u001b[1;32m    268\u001b[0m b \u001b[39m=\u001b[39m b \u001b[39m+\u001b[39m s\n\u001b[0;32m--> 269\u001b[0m n, err_code \u001b[39m=\u001b[39m decoder\u001b[39m.\u001b[39;49mdecode(b)\n\u001b[1;32m    270\u001b[0m \u001b[39mif\u001b[39;00m n \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    271\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# epoch = -1\n",
    "# validate()\n",
    "for epoch in range(N_EPOCHS):\n",
    "    trn_epoch_losses = train()\n",
    "    val_epoch_losses, ccs = validate()\n",
    "    log(epoch, trn_epoch_losses, val_epoch_losses, ccs)\n",
    "torch.save(model.state_dict(), f\"{save_dir}/{MODEL_NAME}_{OPTIMIZER}_{N_EPOCHS}_{LR_INIT}.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('DorsalNet_FC')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "23e725eac71c35374d524a5baa14d2fdf3de38f24c34c7484fb5661a706e502b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
