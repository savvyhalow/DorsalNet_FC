{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms as tf\n",
    "import sys\n",
    "sys.path.append('../code')\n",
    "from dorsalnet import DorsalNet, FC, interpolate_frames\n",
    "from VWAM.utils import SingleImageFolder, iterate_children, hook_model\n",
    "\n",
    "DEVICE = 'cuda:0'\n",
    "DTYPE = torch.bfloat16\n",
    "\n",
    "model = DorsalNet(False, 32).eval().to(DEVICE).to(DTYPE)\n",
    "model.load_state_dict(torch.load('/home/matthew/Data/DorsalNet_FC/base_models/DorsalNet/pretrained.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_FS = 1500\n",
    "DEPTH = 1\n",
    "input_size = (1, 3, 32, 112, 112)\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def choose_downsampling(activations, max_fs):\n",
    "    if activations.ndim == 5:\n",
    "        activations = activations[0:1]\n",
    "        test_range = activations.shape[-1]\n",
    "        numels = np.zeros((test_range+1, test_range))\n",
    "        pbar = tqdm(range(sum(range(test_range+1))))\n",
    "        for k in range(1,test_range+1):\n",
    "            for s in range(1,k+1):\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix_str(f\"testing size {k}, stride {s}\")\n",
    "                n = (activations.shape[-1] - k) / s\n",
    "                if n != int(n):\n",
    "                    continue\n",
    "                else:\n",
    "                    pooled = torch.nn.functional.max_pool3d(activations, kernel_size=(2,k,k), stride=s)\n",
    "                    if pooled.shape[-1] > 1 and pooled.numel() <= max_fs:\n",
    "                        numels[k,s] = pooled.numel()\n",
    "                    else:\n",
    "                        continue\n",
    "        best_k, best_s = np.unravel_index(np.argmax(numels, axis=None), numels.shape)\n",
    "        if (best_k, best_s) == (0,0):\n",
    "            return None\n",
    "        else:\n",
    "            return torch.nn.MaxPool3d(kernel_size=(2, best_k, best_k), stride=best_s)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "layers_dict = iterate_children(model, depth=DEPTH)\n",
    "model = hook_model(model, layers_dict)\n",
    "model(torch.randn(input_size).to(DEVICE).to(DTYPE))\n",
    "\n",
    "layer_downsampling_fns = {}\n",
    "for layer_name, layer_activations in model.activations.items():\n",
    "    print('**************')\n",
    "    print(layer_name)\n",
    "    print('old_shape:', layer_activations.flatten().shape)\n",
    "    layer_downsampling_fn = choose_downsampling(layer_activations, MAX_FS)\n",
    "    layer_downsampling_fns[layer_name] = layer_downsampling_fn\n",
    "    if layer_downsampling_fn is not None:\n",
    "        layer_activations = layer_downsampling_fns[layer_name](layer_activations)\n",
    "    print('new_shape:', layer_activations.flatten().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize FC Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_id = 'S00'\n",
    "trn_brain = np.load(f'/home/matthew/Data/DorsalNet_FC/fMRI_data/{subject_id}/NaturalMovies/trn.npy')\n",
    "trn_brain = torch.tensor(np.nan_to_num(trn_brain), device=DEVICE)\n",
    "n_voxels = trn_brain.shape[1]\n",
    "\n",
    "val_brain = np.load(f'/home/matthew/Data/DorsalNet_FC/fMRI_data/{subject_id}/NaturalMovies/val_rpts.npy')\n",
    "val_brain = torch.tensor(np.nan_to_num(val_brain).mean(0), device=DEVICE)\n",
    "\n",
    "fc = FC(n_voxels).to(DEVICE).to(DTYPE)\n",
    "print(fc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "from torch.optim import Adam\n",
    "\n",
    "def column_corr(A, B, dof=0):\n",
    "    \"\"\"Efficiently compute correlations between columns of two matrices\n",
    "    \n",
    "    Does NOT compute full correlation matrix btw `A` and `B`; returns a \n",
    "    vector of correlation coefficients. FKA ccMatrix.\"\"\"\n",
    "    zs = lambda x: (x-np.nanmean(x, axis=0))/np.nanstd(x, axis=0, ddof=dof)\n",
    "    rTmp = np.nansum(zs(A)*zs(B), axis=0)\n",
    "    n = A.shape[0]\n",
    "    # make sure not to count nans\n",
    "    nNaN = np.sum(np.logical_or(np.isnan(zs(A)), np.isnan(zs(B))), 0)\n",
    "    n = n - nNaN\n",
    "    r = rTmp/n\n",
    "    return r\n",
    "\n",
    "EXPERIMENT = 'NaturalMovies'\n",
    "\n",
    "batch_sizes = {\n",
    "    'NaturalMovies': 30,\n",
    "    'vedb_ver01': 50,\n",
    "}\n",
    "\n",
    "preprocess = tf.Compose([\n",
    "    tf.Resize(112),\n",
    "    tf.ToTensor(),\n",
    "])\n",
    "\n",
    "trn_dl = DataLoader(\n",
    "    SingleImageFolder(f'/home/matthew/Data/DorsalNet_FC/stimuli/{EXPERIMENT}/images/trn', transform=preprocess),\n",
    "    batch_size=batch_sizes[EXPERIMENT], \n",
    "    shuffle=False)\n",
    "\n",
    "val_dl = DataLoader(\n",
    "    SingleImageFolder(f'/home/matthew/Data/DorsalNet_FC/stimuli/{EXPERIMENT}/images/val', transform=preprocess),\n",
    "    batch_size=batch_sizes[EXPERIMENT], \n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "N_EPOCHS = 50\n",
    "LR_INIT = 0.1\n",
    "\n",
    "run = wandb.init(\n",
    "    # Set the project where this run will be logged\n",
    "    project=\"DorsalNet_FC_Pilot\",\n",
    "    # Track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"learning_rate\": LR_INIT,\n",
    "        \"epochs\": N_EPOCHS,\n",
    "        \"experiment\": EXPERIMENT,\n",
    "})\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "optimizer = Adam(fc.parameters(), lr=LR_INIT)\n",
    "for epoch in range(N_EPOCHS):\n",
    "    ### Train\n",
    "    pbar = tqdm(enumerate(trn_dl), total=len(trn_brain), desc=f\"Epoch {epoch} Training\")\n",
    "    epoch_losses = []\n",
    "    for i, batch in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        batch = interpolate_frames(batch, input_size[2])\n",
    "        model.forward(batch.unsqueeze(0).to(DTYPE).to(DEVICE))\n",
    "        all_activations = []\n",
    "        for layer_name, layer_activations in model.activations.items():\n",
    "            layer_downsampling_fn = layer_downsampling_fns[layer_name]\n",
    "            if layer_downsampling_fn is not None:\n",
    "                layer_activations = layer_downsampling_fn(layer_activations)\n",
    "            all_activations.append(layer_activations.mean(0).flatten())\n",
    "            model.activations[layer_name] = 0\n",
    "        fc_out = fc(torch.cat(all_activations).unsqueeze(0))\n",
    "        batch_brain = (trn_brain[min(i+2, len(trn_brain)-1)] + trn_brain[min(i+3, len(trn_brain)-1)]) / 2\n",
    "        loss = torch.square(fc_out[0]/1000 - batch_brain).sum().sqrt()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_losses.append(loss.item())\n",
    "        pbar.set_postfix_str(f\"Mean Epoch Loss: {torch.mean(torch.tensor(epoch_losses)).item():.2f}\")\n",
    "    ### Evaluate\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(enumerate(val_dl), total=len(val_brain), desc=f\"Epoch {epoch} Evaluation\")\n",
    "        val_outputs = []\n",
    "        for i, batch in pbar:\n",
    "            batch = interpolate_frames(batch, input_size[2])\n",
    "            model.forward(batch.unsqueeze(0).to(DTYPE).to(DEVICE))\n",
    "            all_activations = []\n",
    "            for layer_name, layer_activations in model.activations.items():\n",
    "                layer_downsampling_fn = layer_downsampling_fns[layer_name]\n",
    "                if layer_downsampling_fn is not None:\n",
    "                    layer_activations = layer_downsampling_fn(layer_activations)\n",
    "                all_activations.append(layer_activations.mean(0).flatten())\n",
    "                model.activations[layer_name] = 0\n",
    "            fc_out = fc(torch.cat(all_activations).unsqueeze(0))\n",
    "            val_outputs.append(fc_out.cpu().float().numpy())\n",
    "        ccs = column_corr(np.concatenate(val_outputs), val_brain.cpu().numpy())\n",
    "        print(f\"Mean Prediction Accuracy: {ccs.mean():.2f}\")\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch,\n",
    "        \"trn_loss\": torch.mean(torch.tensor(epoch_losses)).item(),\n",
    "        \"val_acc\": ccs.mean(),\n",
    "    })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('DorsalNet_FC')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "23e725eac71c35374d524a5baa14d2fdf3de38f24c34c7484fb5661a706e502b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
