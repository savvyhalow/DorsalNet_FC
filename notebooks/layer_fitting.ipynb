{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matthew/anaconda3/envs/DorsalNet_FC/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/matthew/anaconda3/envs/DorsalNet_FC/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "Using cache found in /home/matthew/.cache/torch/hub/pytorch_vision_v0.6.0\n",
      "/home/matthew/anaconda3/envs/DorsalNet_FC/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/matthew/anaconda3/envs/DorsalNet_FC/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms as tf\n",
    "import sys\n",
    "sys.path.append('../code')\n",
    "from dorsalnet import DorsalNet, FC, interpolate_frames\n",
    "from VWAM.utils import SingleImageFolder, iterate_children, hook_model\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "DEVICE = 'cuda:1'\n",
    "DTYPE = torch.bfloat16\n",
    "MODEL_NAME = 'alexnet'\n",
    "OPTIMIZER = 'Adam'\n",
    "EXPERIMENT = 'NaturalMovies'\n",
    "N_EPOCHS = 50\n",
    "LR_INIT = 1e-3\n",
    "\n",
    "if MODEL_NAME.lower() == 'dorsalnet':\n",
    "    DEPTH = 1\n",
    "    INPUT_SIZE = (1, 3, 32, 112, 112)\n",
    "    preprocess = tf.Compose([\n",
    "        tf.Resize(112),\n",
    "        tf.ToTensor(),\n",
    "    ])\n",
    "    image_augmentations = tf.Compose([\n",
    "        tf.RandomCrop(112, padding=4),\n",
    "        tf.RandomRotation(10),\n",
    "        tf.RandomCrop(112, padding=3),\n",
    "    ])\n",
    "    model = DorsalNet(False, 32).eval().to(DEVICE).to(DTYPE)\n",
    "    model.load_state_dict(torch.load('/home/matthew/Data/DorsalNet_FC/base_models/DorsalNet/pretrained.pth'))\n",
    "else:\n",
    "    DEPTH = 2\n",
    "    INPUT_SIZE = (1, 3, 224, 224)\n",
    "    preprocess = tf.Compose([\n",
    "        tf.Resize(256),\n",
    "        tf.CenterCrop(224),\n",
    "        tf.ToTensor(),\n",
    "        tf.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    image_augmentations = tf.Compose([\n",
    "        tf.RandomCrop(224, padding=4),\n",
    "        tf.RandomRotation(10),\n",
    "        tf.RandomCrop(224, padding=3),\n",
    "    ])\n",
    "    if MODEL_NAME.lower() == 'alexnet':\n",
    "        model = torch.hub.load('pytorch/vision:v0.6.0', 'alexnet', pretrained=True).eval().to(DEVICE).to(DTYPE)\n",
    "    elif MODEL_NAME.lower() == 'inception_v3':\n",
    "        model = torch.hub.load('pytorch/vision:v0.6.0', 'inception_v3', pretrained=True).eval().to(DEVICE).to(DTYPE)\n",
    "    elif MODEL_NAME.lower() == 'vgg16':\n",
    "        model = torch.hub.load('pytorch/vision:v0.6.0', 'vgg16', pretrained=True).eval().to(DEVICE).to(DTYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************\n",
      "model.features.0\n",
      "old_shape: torch.Size([1, 64, 224, 224])\n",
      "old # activations: torch.Size([3211264])\n",
      "new_shape: torch.Size([1, 64, 8, 8])\n",
      "new # activations: torch.Size([4096])\n",
      "**************\n",
      "model.features.1\n",
      "old_shape: torch.Size([1, 64, 224, 224])\n",
      "old # activations: torch.Size([3211264])\n",
      "new_shape: torch.Size([1, 64, 8, 8])\n",
      "new # activations: torch.Size([4096])\n",
      "**************\n",
      "model.features.2\n",
      "old_shape: torch.Size([1, 64, 224, 224])\n",
      "old # activations: torch.Size([3211264])\n",
      "new_shape: torch.Size([1, 64, 8, 8])\n",
      "new # activations: torch.Size([4096])\n",
      "**************\n",
      "model.features.3\n",
      "old_shape: torch.Size([1, 64, 224, 224])\n",
      "old # activations: torch.Size([3211264])\n",
      "new_shape: torch.Size([1, 64, 8, 8])\n",
      "new # activations: torch.Size([4096])\n",
      "**************\n",
      "model.features.4\n",
      "old_shape: torch.Size([1, 64, 112, 112])\n",
      "old # activations: torch.Size([802816])\n",
      "new_shape: torch.Size([1, 64, 8, 8])\n",
      "new # activations: torch.Size([4096])\n",
      "**************\n",
      "model.features.5\n",
      "old_shape: torch.Size([1, 128, 112, 112])\n",
      "old # activations: torch.Size([1605632])\n",
      "new_shape: torch.Size([1, 128, 6, 6])\n",
      "new # activations: torch.Size([4608])\n",
      "**************\n",
      "model.features.6\n",
      "old_shape: torch.Size([1, 128, 112, 112])\n",
      "old # activations: torch.Size([1605632])\n",
      "new_shape: torch.Size([1, 128, 6, 6])\n",
      "new # activations: torch.Size([4608])\n",
      "**************\n",
      "model.features.7\n",
      "old_shape: torch.Size([1, 128, 112, 112])\n",
      "old # activations: torch.Size([1605632])\n",
      "new_shape: torch.Size([1, 128, 6, 6])\n",
      "new # activations: torch.Size([4608])\n",
      "**************\n",
      "model.features.8\n",
      "old_shape: torch.Size([1, 128, 112, 112])\n",
      "old # activations: torch.Size([1605632])\n",
      "new_shape: torch.Size([1, 128, 6, 6])\n",
      "new # activations: torch.Size([4608])\n",
      "**************\n",
      "model.features.9\n",
      "old_shape: torch.Size([1, 128, 56, 56])\n",
      "old # activations: torch.Size([401408])\n",
      "new_shape: torch.Size([1, 128, 6, 6])\n",
      "new # activations: torch.Size([4608])\n",
      "**************\n",
      "model.features.10\n",
      "old_shape: torch.Size([1, 256, 56, 56])\n",
      "old # activations: torch.Size([802816])\n",
      "new_shape: torch.Size([1, 256, 4, 4])\n",
      "new # activations: torch.Size([4096])\n",
      "**************\n",
      "model.features.11\n",
      "old_shape: torch.Size([1, 256, 56, 56])\n",
      "old # activations: torch.Size([802816])\n",
      "new_shape: torch.Size([1, 256, 4, 4])\n",
      "new # activations: torch.Size([4096])\n",
      "**************\n",
      "model.features.12\n",
      "old_shape: torch.Size([1, 256, 56, 56])\n",
      "old # activations: torch.Size([802816])\n",
      "new_shape: torch.Size([1, 256, 4, 4])\n",
      "new # activations: torch.Size([4096])\n",
      "**************\n",
      "model.features.13\n",
      "old_shape: torch.Size([1, 256, 56, 56])\n",
      "old # activations: torch.Size([802816])\n",
      "new_shape: torch.Size([1, 256, 4, 4])\n",
      "new # activations: torch.Size([4096])\n",
      "**************\n",
      "model.features.14\n",
      "old_shape: torch.Size([1, 256, 56, 56])\n",
      "old # activations: torch.Size([802816])\n",
      "new_shape: torch.Size([1, 256, 4, 4])\n",
      "new # activations: torch.Size([4096])\n",
      "**************\n",
      "model.features.15\n",
      "old_shape: torch.Size([1, 256, 56, 56])\n",
      "old # activations: torch.Size([802816])\n",
      "new_shape: torch.Size([1, 256, 4, 4])\n",
      "new # activations: torch.Size([4096])\n",
      "**************\n",
      "model.features.16\n",
      "old_shape: torch.Size([1, 256, 28, 28])\n",
      "old # activations: torch.Size([200704])\n",
      "new_shape: torch.Size([1, 256, 4, 4])\n",
      "new # activations: torch.Size([4096])\n",
      "**************\n",
      "model.features.17\n",
      "old_shape: torch.Size([1, 512, 28, 28])\n",
      "old # activations: torch.Size([401408])\n",
      "new_shape: torch.Size([1, 512, 3, 3])\n",
      "new # activations: torch.Size([4608])\n",
      "**************\n",
      "model.features.18\n",
      "old_shape: torch.Size([1, 512, 28, 28])\n",
      "old # activations: torch.Size([401408])\n",
      "new_shape: torch.Size([1, 512, 3, 3])\n",
      "new # activations: torch.Size([4608])\n",
      "**************\n",
      "model.features.19\n",
      "old_shape: torch.Size([1, 512, 28, 28])\n",
      "old # activations: torch.Size([401408])\n",
      "new_shape: torch.Size([1, 512, 3, 3])\n",
      "new # activations: torch.Size([4608])\n",
      "**************\n",
      "model.features.20\n",
      "old_shape: torch.Size([1, 512, 28, 28])\n",
      "old # activations: torch.Size([401408])\n",
      "new_shape: torch.Size([1, 512, 3, 3])\n",
      "new # activations: torch.Size([4608])\n",
      "**************\n",
      "model.features.21\n",
      "old_shape: torch.Size([1, 512, 28, 28])\n",
      "old # activations: torch.Size([401408])\n",
      "new_shape: torch.Size([1, 512, 3, 3])\n",
      "new # activations: torch.Size([4608])\n",
      "**************\n",
      "model.features.22\n",
      "old_shape: torch.Size([1, 512, 28, 28])\n",
      "old # activations: torch.Size([401408])\n",
      "new_shape: torch.Size([1, 512, 3, 3])\n",
      "new # activations: torch.Size([4608])\n",
      "**************\n",
      "model.features.23\n",
      "old_shape: torch.Size([1, 512, 14, 14])\n",
      "old # activations: torch.Size([100352])\n",
      "new_shape: torch.Size([1, 512, 3, 3])\n",
      "new # activations: torch.Size([4608])\n",
      "**************\n",
      "model.features.24\n",
      "old_shape: torch.Size([1, 512, 14, 14])\n",
      "old # activations: torch.Size([100352])\n",
      "new_shape: torch.Size([1, 512, 3, 3])\n",
      "new # activations: torch.Size([4608])\n",
      "**************\n",
      "model.features.25\n",
      "old_shape: torch.Size([1, 512, 14, 14])\n",
      "old # activations: torch.Size([100352])\n",
      "new_shape: torch.Size([1, 512, 3, 3])\n",
      "new # activations: torch.Size([4608])\n",
      "**************\n",
      "model.features.26\n",
      "old_shape: torch.Size([1, 512, 14, 14])\n",
      "old # activations: torch.Size([100352])\n",
      "new_shape: torch.Size([1, 512, 3, 3])\n",
      "new # activations: torch.Size([4608])\n",
      "**************\n",
      "model.features.27\n",
      "old_shape: torch.Size([1, 512, 14, 14])\n",
      "old # activations: torch.Size([100352])\n",
      "new_shape: torch.Size([1, 512, 3, 3])\n",
      "new # activations: torch.Size([4608])\n",
      "**************\n",
      "model.features.28\n",
      "old_shape: torch.Size([1, 512, 14, 14])\n",
      "old # activations: torch.Size([100352])\n",
      "new_shape: torch.Size([1, 512, 3, 3])\n",
      "new # activations: torch.Size([4608])\n",
      "**************\n",
      "model.features.29\n",
      "old_shape: torch.Size([1, 512, 14, 14])\n",
      "old # activations: torch.Size([100352])\n",
      "new_shape: torch.Size([1, 512, 3, 3])\n",
      "new # activations: torch.Size([4608])\n",
      "**************\n",
      "model.features.30\n",
      "old_shape: torch.Size([1, 512, 7, 7])\n",
      "old # activations: torch.Size([25088])\n",
      "new_shape: torch.Size([1, 512, 3, 3])\n",
      "new # activations: torch.Size([4608])\n",
      "**************\n",
      "model.avgpool\n",
      "old_shape: torch.Size([1, 512, 7, 7])\n",
      "old # activations: torch.Size([25088])\n",
      "new_shape: torch.Size([1, 512, 3, 3])\n",
      "new # activations: torch.Size([4608])\n",
      "**************\n",
      "model.classifier.0\n",
      "old_shape: torch.Size([1, 4096])\n",
      "old # activations: torch.Size([4096])\n",
      "new_shape: torch.Size([1, 4096])\n",
      "new # activations: torch.Size([4096])\n",
      "**************\n",
      "model.classifier.1\n",
      "old_shape: torch.Size([1, 4096])\n",
      "old # activations: torch.Size([4096])\n",
      "new_shape: torch.Size([1, 4096])\n",
      "new # activations: torch.Size([4096])\n",
      "**************\n",
      "model.classifier.2\n",
      "old_shape: torch.Size([1, 4096])\n",
      "old # activations: torch.Size([4096])\n",
      "new_shape: torch.Size([1, 4096])\n",
      "new # activations: torch.Size([4096])\n",
      "**************\n",
      "model.classifier.3\n",
      "old_shape: torch.Size([1, 4096])\n",
      "old # activations: torch.Size([4096])\n",
      "new_shape: torch.Size([1, 4096])\n",
      "new # activations: torch.Size([4096])\n",
      "**************\n",
      "model.classifier.4\n",
      "old_shape: torch.Size([1, 4096])\n",
      "old # activations: torch.Size([4096])\n",
      "new_shape: torch.Size([1, 4096])\n",
      "new # activations: torch.Size([4096])\n",
      "**************\n",
      "model.classifier.5\n",
      "old_shape: torch.Size([1, 4096])\n",
      "old # activations: torch.Size([4096])\n",
      "new_shape: torch.Size([1, 4096])\n",
      "new # activations: torch.Size([4096])\n",
      "**************\n",
      "model.classifier.6\n",
      "old_shape: torch.Size([1, 1000])\n",
      "old # activations: torch.Size([1000])\n",
      "new_shape: torch.Size([1, 1000])\n",
      "new # activations: torch.Size([1000])\n"
     ]
    }
   ],
   "source": [
    "MAX_FS = 5000\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "# from tqdm.notebook import tqdm\n",
    "\n",
    "def choose_downsampling(activations, max_fs):\n",
    "    num_channels = activations.shape[1]\n",
    "    if activations.ndim == 4:\n",
    "        max_output_dim = int((max_fs / num_channels)**(1/2))\n",
    "        return torch.nn.AdaptiveMaxPool2d(max_output_dim)\n",
    "    elif activations.ndim == 5:\n",
    "        max_output_dim = int((max_fs / num_channels)**(1/3))\n",
    "        return torch.nn.AdaptiveMaxPool3d(max_output_dim)\n",
    "\n",
    "layers_dict = iterate_children(model, depth=DEPTH)\n",
    "layers_dict = {layer_name: ds_function for layer_name, ds_function in layers_dict.items() if 'dropout' not in layer_name and 'concat' not in layer_name}\n",
    "model = hook_model(model, layers_dict)\n",
    "model(torch.randn(INPUT_SIZE).to(DEVICE).to(DTYPE))\n",
    "\n",
    "layer_downsampling_fns = {}\n",
    "for layer_name, layer_activations in model.activations.items():\n",
    "    layer_activations = layer_activations\n",
    "    print('**************')\n",
    "    print(layer_name)\n",
    "    print('old_shape:', layer_activations.shape)\n",
    "    print('old # activations:', layer_activations.flatten().shape)\n",
    "    layer_downsampling_fn = choose_downsampling(layer_activations, MAX_FS)\n",
    "    layer_downsampling_fns[layer_name] = layer_downsampling_fn\n",
    "    if layer_downsampling_fn is not None:\n",
    "        layer_activations = layer_downsampling_fns[layer_name](layer_activations)\n",
    "    print('new_shape:', layer_activations.shape)\n",
    "    print('new # activations:', layer_activations.flatten().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize FC Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FC(\n",
      "  (linear): LazyLinear(in_features=0, out_features=9853, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matthew/.local/lib/python3.10/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "SUBJECT_ID = 'S00'\n",
    "trn_brain = np.load(f'/home/matthew/Data/DorsalNet_FC/fMRI_data/{SUBJECT_ID}/NaturalMovies/trn.npy')\n",
    "trn_brain = torch.tensor(np.nan_to_num(trn_brain), device=DEVICE)\n",
    "n_voxels = trn_brain.shape[1]\n",
    "\n",
    "val_brain = np.load(f'/home/matthew/Data/DorsalNet_FC/fMRI_data/{SUBJECT_ID}/NaturalMovies/val_rpts.npy')\n",
    "val_brain = torch.tensor(np.nan_to_num(val_brain).mean(0), device=DEVICE)\n",
    "\n",
    "fc = FC(n_voxels).to(DEVICE).to(DTYPE)\n",
    "print(fc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "from torch.optim import Adam\n",
    "\n",
    "def column_corr(A, B, dof=0):\n",
    "    \"\"\"Efficiently compute correlations between columns of two matrices\n",
    "    \n",
    "    Does NOT compute full correlation matrix btw `A` and `B`; returns a \n",
    "    vector of correlation coefficients. FKA ccMatrix.\"\"\"\n",
    "    zs = lambda x: (x-np.nanmean(x, axis=0))/np.nanstd(x, axis=0, ddof=dof)\n",
    "    rTmp = np.nansum(zs(A)*zs(B), axis=0)\n",
    "    n = A.shape[0]\n",
    "    # make sure not to count nans\n",
    "    nNaN = np.sum(np.logical_or(np.isnan(zs(A)), np.isnan(zs(B))), 0)\n",
    "    n = n - nNaN\n",
    "    r = rTmp/n\n",
    "    return r\n",
    "\n",
    "\n",
    "batch_sizes = {\n",
    "    'NaturalMovies': 30,\n",
    "    'vedb_ver01': 50,\n",
    "}\n",
    "\n",
    "trn_dl = DataLoader(\n",
    "    SingleImageFolder(f'/home/matthew/Data/DorsalNet_FC/stimuli/{EXPERIMENT}/images/trn', transform=preprocess),\n",
    "    batch_size=batch_sizes[EXPERIMENT], \n",
    "    shuffle=False)\n",
    "\n",
    "val_dl = DataLoader(\n",
    "    SingleImageFolder(f'/home/matthew/Data/DorsalNet_FC/stimuli/{EXPERIMENT}/images/val', transform=preprocess),\n",
    "    batch_size=batch_sizes[EXPERIMENT], \n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmshinkle\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/matthew/Code/DorsalNet_FC/notebooks/wandb/run-20230428_095745-nli5y5ep</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mshinkle/DorsalNet_FC_Pilot/runs/nli5y5ep' target=\"_blank\">dutiful-flower-69</a></strong> to <a href='https://wandb.ai/mshinkle/DorsalNet_FC_Pilot' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mshinkle/DorsalNet_FC_Pilot' target=\"_blank\">https://wandb.ai/mshinkle/DorsalNet_FC_Pilot</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mshinkle/DorsalNet_FC_Pilot/runs/nli5y5ep' target=\"_blank\">https://wandb.ai/mshinkle/DorsalNet_FC_Pilot/runs/nli5y5ep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "run = wandb.init(\n",
    "    # Set the project where this run will be logged\n",
    "    project=\"DorsalNet_FC_Pilot\",\n",
    "    # Track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"model_name\": MODEL_NAME,\n",
    "        \"experiment\": EXPERIMENT,\n",
    "        \"subject_id\": SUBJECT_ID,\n",
    "        \"optimizer\": OPTIMIZER,\n",
    "        \"epochs\": N_EPOCHS,\n",
    "        \"learning_rate\": LR_INIT,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "if OPTIMIZER == 'SGD':\n",
    "    optimizer = torch.optim.SGD(fc.parameters(), lr=LR_INIT)\n",
    "elif OPTIMIZER == 'Adam':\n",
    "    optimizer = torch.optim.Adam(fc.parameters(), lr=LR_INIT)\n",
    "\n",
    "def train():\n",
    "    pbar = tqdm(enumerate(trn_dl), total=len(trn_brain), desc=f\"Epoch {epoch} Training\")\n",
    "    trn_epoch_losses = []\n",
    "    for i, batch in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        if MODEL_NAME.lower() == 'DorsalNet':\n",
    "            batch = interpolate_frames(batch, INPUT_SIZE[2]).unsqueeze(0)\n",
    "        model.forward(image_augmentations(batch).to(DTYPE).to(DEVICE))\n",
    "        all_activations = []\n",
    "        for layer_name, layer_activations in model.activations.items():\n",
    "            layer_downsampling_fn = layer_downsampling_fns[layer_name]\n",
    "            if layer_downsampling_fn is not None:\n",
    "                layer_activations = layer_downsampling_fn(layer_activations)\n",
    "            all_activations.append(layer_activations.mean(0).flatten())\n",
    "            model.activations[layer_name] = 0\n",
    "        fc_out = fc(torch.cat(all_activations).unsqueeze(0))\n",
    "        batch_brain = (trn_brain[min(i+2, len(trn_brain)-1)] + trn_brain[min(i+3, len(trn_brain)-1)]) / 2\n",
    "        loss = torch.square(fc_out[0]/1000 - batch_brain).sum().sqrt()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        trn_epoch_losses.append(loss.item())\n",
    "        pbar.set_postfix_str(f\"Mean Epoch Loss: {torch.mean(torch.tensor(trn_epoch_losses)).item():.2f}\")\n",
    "    return trn_epoch_losses\n",
    "\n",
    "def validate():\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(enumerate(val_dl), total=len(val_brain), desc=f\"Epoch {epoch} Validation\")\n",
    "        val_outputs = []\n",
    "        val_epoch_losses = []\n",
    "        for i, batch in pbar:\n",
    "            if MODEL_NAME.lower() == 'DorsalNet':\n",
    "                batch = interpolate_frames(batch, INPUT_SIZE[2]).unsqueeze(0)\n",
    "            model.forward(batch.unsqueeze(DTYPE).to(DEVICE))\n",
    "            all_activations = []\n",
    "            for layer_name, layer_activations in model.activations.items():\n",
    "                layer_downsampling_fn = layer_downsampling_fns[layer_name]\n",
    "                if layer_downsampling_fn is not None:\n",
    "                    layer_activations = layer_downsampling_fn(layer_activations)\n",
    "                all_activations.append(layer_activations.mean(0).flatten())\n",
    "                model.activations[layer_name] = 0\n",
    "            fc_out = fc(torch.cat(all_activations).unsqueeze(0))\n",
    "            batch_brain = (val_brain[min(i+2, len(val_brain)-1)] + val_brain[min(i+3, len(val_brain)-1)]) / 2\n",
    "            loss = torch.square(fc_out[0]/1000 - batch_brain).sum().sqrt()\n",
    "            val_outputs.append(fc_out.cpu().float().numpy())\n",
    "            val_epoch_losses.append(loss.item())\n",
    "            pbar.set_postfix_str(f\"Mean Epoch Loss: {torch.mean(torch.tensor(val_epoch_losses)).item():.2f}\")\n",
    "        ccs = column_corr(np.concatenate(val_outputs), val_brain.cpu().numpy())\n",
    "        print(f\"Mean Prediction Accuracy: {ccs.mean():.2f}\")\n",
    "    return val_epoch_losses, ccs\n",
    "        \n",
    "def log(epoch, trn_epoch_losses, val_epoch_losses, ccs):\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch,\n",
    "        \"trn_loss\": torch.mean(torch.tensor(trn_epoch_losses)).item(),\n",
    "        \"val_loss\": torch.mean(torch.tensor(val_epoch_losses)).item(),\n",
    "        \"val_acc\": ccs.mean(),\n",
    "    })\n",
    "\n",
    "save_dir = f'/home/matthew/Data/DorsalNet_FC/fits/{EXPERIMENT}/{SUBJECT_ID}'\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 Training:   0%|          | 0/3600 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 3.06 GiB (GPU 1; 11.91 GiB total capacity; 9.91 GiB already allocated; 1.04 GiB free; 10.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# epoch = -1\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# validate()\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(N_EPOCHS):\n\u001b[0;32m----> 4\u001b[0m     trn_epoch_losses \u001b[39m=\u001b[39m train()\n\u001b[1;32m      5\u001b[0m     val_epoch_losses, ccs \u001b[39m=\u001b[39m validate()\n\u001b[1;32m      6\u001b[0m     log(epoch, trn_epoch_losses, val_epoch_losses, ccs)\n",
      "Cell \u001b[0;32mIn[6], line 26\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msquare(fc_out[\u001b[39m0\u001b[39m]\u001b[39m/\u001b[39m\u001b[39m1000\u001b[39m \u001b[39m-\u001b[39m batch_brain)\u001b[39m.\u001b[39msum()\u001b[39m.\u001b[39msqrt()\n\u001b[1;32m     25\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> 26\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     27\u001b[0m trn_epoch_losses\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n\u001b[1;32m     28\u001b[0m pbar\u001b[39m.\u001b[39mset_postfix_str(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMean Epoch Loss: \u001b[39m\u001b[39m{\u001b[39;00mtorch\u001b[39m.\u001b[39mmean(torch\u001b[39m.\u001b[39mtensor(trn_epoch_losses))\u001b[39m.\u001b[39mitem()\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     34\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:132\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    129\u001b[0m     state_steps \u001b[39m=\u001b[39m []\n\u001b[1;32m    130\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m'\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m--> 132\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_init_group(\n\u001b[1;32m    133\u001b[0m         group,\n\u001b[1;32m    134\u001b[0m         params_with_grad,\n\u001b[1;32m    135\u001b[0m         grads,\n\u001b[1;32m    136\u001b[0m         exp_avgs,\n\u001b[1;32m    137\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    138\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    139\u001b[0m         state_steps)\n\u001b[1;32m    141\u001b[0m     adam(\n\u001b[1;32m    142\u001b[0m         params_with_grad,\n\u001b[1;32m    143\u001b[0m         grads,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m         found_inf\u001b[39m=\u001b[39m\u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfound_inf\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m),\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:94\u001b[0m, in \u001b[0;36mAdam._init_group\u001b[0;34m(self, group, params_with_grad, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps)\u001b[0m\n\u001b[1;32m     92\u001b[0m state[\u001b[39m'\u001b[39m\u001b[39mexp_avg\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros_like(p, memory_format\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mpreserve_format)\n\u001b[1;32m     93\u001b[0m \u001b[39m# Exponential moving average of squared gradient values\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m state[\u001b[39m'\u001b[39m\u001b[39mexp_avg_sq\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mzeros_like(p, memory_format\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mpreserve_format)\n\u001b[1;32m     95\u001b[0m \u001b[39mif\u001b[39;00m group[\u001b[39m'\u001b[39m\u001b[39mamsgrad\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m     96\u001b[0m     \u001b[39m# Maintains max of all exp. moving avg. of sq. grad. values\u001b[39;00m\n\u001b[1;32m     97\u001b[0m     state[\u001b[39m'\u001b[39m\u001b[39mmax_exp_avg_sq\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros_like(p, memory_format\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mpreserve_format)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 3.06 GiB (GPU 1; 11.91 GiB total capacity; 9.91 GiB already allocated; 1.04 GiB free; 10.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# epoch = -1\n",
    "# validate()\n",
    "for epoch in range(N_EPOCHS):\n",
    "    trn_epoch_losses = train()\n",
    "    val_epoch_losses, ccs = validate()\n",
    "    log(epoch, trn_epoch_losses, val_epoch_losses, ccs)\n",
    "torch.save(model.state_dict(), f\"{save_dir}/{MODEL_NAME}_{OPTIMIZER}_{N_EPOCHS}_{LR_INIT}.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('DorsalNet_FC')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "23e725eac71c35374d524a5baa14d2fdf3de38f24c34c7484fb5661a706e502b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
